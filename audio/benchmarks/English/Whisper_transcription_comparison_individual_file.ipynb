{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bc4508-c69d-4bfa-9557-76082b16c03b",
   "metadata": {},
   "source": [
    "# Parent-Infant Interaction Analysis Pipeline\n",
    "\n",
    "## Setup and Dependencies\n",
    "```python\n",
    "import torch\n",
    "import whisper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "from scipy.signal import butter, filtfilt\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import soundfile as sf\n",
    "from typing import Union, List, Tuple\n",
    "```\n",
    "Key dependencies:\n",
    "- OpenAI Whisper for speech recognition\n",
    "- PyDub for audio processing\n",
    "- FFmpeg for audio file manipulation\n",
    "- Pandas for data handling\n",
    "- NumPy for numerical operations\n",
    "\n",
    "## Pipeline Overview\n",
    "This notebook implements a three-stage pipeline for analyzing parent-infant interactions:\n",
    "1. Audio Preprocessing\n",
    "2. Speech Recognition\n",
    "3. Accuracy Analysis\n",
    "\n",
    "### 1. Audio Preprocessing (`AudioPreprocessor` class)\n",
    "Prepares audio files for optimal speech recognition:\n",
    "- Converts stereo to mono (required for Whisper)\n",
    "- Adjusts sample rate to 16kHz\n",
    "- Normalizes volume\n",
    "- Optional silence removal (not recommended for interaction analysis)\n",
    "\n",
    "Example usage:\n",
    "```python\n",
    "preprocessor = AudioPreprocessor(input_file)\n",
    "processed_file = preprocessor.process_audio(\n",
    "    output_dir=output_dir,\n",
    "    remove_silence=False  # Keep natural pauses\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. Speech Recognition (`transcribe_audio` function)\n",
    "Uses Whisper ASR with parent-infant specific settings:\n",
    "- Large-v2 model for accuracy\n",
    "- Custom prompt for infant vocalizations\n",
    "- Word-level timestamps\n",
    "- Speaker detection capabilities\n",
    "\n",
    "Output includes:\n",
    "- Transcribed text\n",
    "- Timing information\n",
    "- Optional speaker labels\n",
    "\n",
    "### 3. Accuracy Analysis (`EnhancedTranscriptionComparator` class)\n",
    "Compares Whisper output with human transcriptions:\n",
    "- Text similarity scoring\n",
    "- Word-level accuracy metrics\n",
    "- Detailed analysis reports\n",
    "\n",
    "### Key Metrics\n",
    "1. Text Similarity\n",
    "   - SequenceMatcher score (Ratcliff/Obershelp algorithm)\n",
    "   - Formula: ratio = 2.0 * M / T\n",
    "     * M = sum of lengths of matched subsequences\n",
    "     * T = total length of both strings combined\n",
    "\n",
    "2. Word-level Analysis\n",
    "   - Word count comparison\n",
    "   - Correct words identification\n",
    "   - Mismatch detection\n",
    "   - Accuracy percentage calculation\n",
    "\n",
    "### Output Format\n",
    "CSV file with columns:\n",
    "- Time Range\n",
    "- Whisper Transcription\n",
    "- Human Transcription\n",
    "- SequenceMatcher Score\n",
    "- Word Count (Human)\n",
    "- Word Count (Whisper)\n",
    "- Correct Words\n",
    "- Mismatches\n",
    "- Accuracy (%)\n",
    "- Comments/Notes\n",
    "\n",
    "## Results Interpretation\n",
    "The comparison output provides:\n",
    "- Time-aligned transcriptions\n",
    "- Word-level accuracy metrics\n",
    "- Quality indicators\n",
    "- Automated issue detection\n",
    "\n",
    "Example metrics from current run:\n",
    "- Total segments: 95\n",
    "- Average accuracy: 65.3%\n",
    "- High quality matches: 39\n",
    "- Low quality matches: 38\n",
    "\n",
    "## Future Improvements\n",
    "Consider:\n",
    "- GPU acceleration for faster processing\n",
    "- Enhanced speaker detection\n",
    "- Controlled vocabulary for infant sounds\n",
    "- Batch processing capabilities\n",
    "\n",
    "## Suggested File Structure\n",
    "\n",
    "```\n",
    "File Structure Overview:\n",
    "project/wearable/\n",
    "├── media/[MONTH]/                    # Audio files\n",
    "│   ├── IW_[ID]_[MONTH]_TL.wav       # Original audio\n",
    "│   └── processed/                    # Processed audio files\n",
    "│       ├── IW_[ID]_[MONTH]_TL_mono.wav\n",
    "│       ├── IW_[ID]_[MONTH]_TL_mono_16k.wav\n",
    "│       └── IW_[ID]_[MONTH]_TL_mono_16k_normalized.wav\n",
    "│\n",
    "└── transcription/[MONTH]/            # Transcription files\n",
    "    ├── IW_[ID]_[MONTH]_whisper_results_without_speaker.csv  # Whisper output\n",
    "    └── IW_[ID]_[MONTH]_transcription_comparison.csv         # Analysis results\n",
    "\n",
    "Naming Convention:\n",
    "- [ID]: Participant ID (e.g., 002)\n",
    "- [MONTH]: Recording month (e.g., 12mon)\n",
    "```\n",
    "\n",
    "## Usage Workflow\n",
    "\n",
    "### 1. Preprocess Audio\n",
    "```python\n",
    "# Initialize and process audio\n",
    "preprocessor = AudioPreprocessor(\"/path/to/media/12mon/IW_002_12_TL.wav\")\n",
    "processed_file = preprocessor.process_audio(\n",
    "    output_dir=\"/path/to/media/12mon/processed\",\n",
    "    remove_silence=False    # Keep silence for interaction analysis\n",
    ")\n",
    "# Output: IW_002_12_TL_mono_16k_normalized.wav\n",
    "```\n",
    "\n",
    "### 2. Transcribe Audio\n",
    "```python\n",
    "# Run Whisper transcription\n",
    "df, result = transcribe_audio(\n",
    "    audio_path=\"/path/to/processed/IW_002_12_TL_mono_16k_normalized.wav\",\n",
    "    speaker_hints=[\"[Mother]\", \"[Father]\", \"[Baby]\"]\n",
    ")\n",
    "df.to_csv(\"002_12_whisper_results.csv\", index=False)\n",
    "```\n",
    "\n",
    "### 3. Compare Transcriptions\n",
    "```python\n",
    "# Compare with human transcription\n",
    "results_df, stats = compare_and_export(\n",
    "    whisper_df,    # Whisper output\n",
    "    human_df,      # Human transcription\n",
    "    \"002_12_transcription_comparison.csv\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Expected Outputs\n",
    "1. Processed Audio: `*_mono_16k_normalized.wav`\n",
    "2. Whisper Output: `*_whisper_results.csv`\n",
    "3. Analysis: `*_transcription_comparison.csv`\n",
    "\n",
    "\n",
    "## Technical Notes\n",
    "- Time tolerance: 0.5 seconds for segment matching\n",
    "- Similarity threshold: 0.3 for considering matches\n",
    "- Text cleaning includes:\n",
    "  * Case normalization\n",
    "  * Punctuation removal\n",
    "  * Common transcription variant standardization\n",
    "- High quality matches: Accuracy > 80%\n",
    "- Low quality matches: Accuracy < 60%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193fc90b-898a-4525-8bac-8ec64cc61401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 18:52:06,537 - INFO - Running FFmpeg command:\n",
      "2025-09-04 18:52:06,538 - INFO - ffmpeg -i /Users/stephanieluo/Downloads/audio_20250811_103358_001.wav -af aresample=16000,aformat=sample_fmts=s16:channel_layouts=mono,anlmdn=s=0.01,volume=1.5 -ar 16000 -ac 1 -y /Users/stephanieluo/Downloads/audio_20250811_103358_001_processed.wav\n",
      "2025-09-04 18:52:14,290 - INFO - Successfully processed audio to: /Users/stephanieluo/Downloads/audio_20250811_103358_001_processed.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: /Users/stephanieluo/Downloads/audio_20250811_103358_001_processed.wav\n"
     ]
    }
   ],
   "source": [
    "###preprocessing\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, input_path: Union[str, Path]):\n",
    "        \"\"\"Initialize audio preprocessor with input file path\"\"\"\n",
    "        self.input_path = Path(input_path)\n",
    "        self.setup_logging()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging configuration\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def process_audio(self, \n",
    "                     output_dir: Union[str, Path],\n",
    "                     apply_noise_reduction: bool = True,\n",
    "                     noise_reduction_strength: float = 0.21,\n",
    "                     target_volume: float = 1.5) -> Path:\n",
    "        \"\"\"\n",
    "        Process audio using FFmpeg filter chain:\n",
    "        1. Convert to mono\n",
    "        2. Adjust sample rate to 16kHz\n",
    "        3. Optional noise reduction\n",
    "        4. Normalize volume\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save processed audio\n",
    "            apply_noise_reduction: Whether to apply noise reduction\n",
    "            noise_reduction_strength: Strength of noise reduction (0.01 to 1.0)\n",
    "            target_volume: Target volume for normalization\n",
    "            \n",
    "        Returns:\n",
    "            Path to processed audio file\n",
    "        \"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Construct output path\n",
    "        output_path = output_dir / f\"{self.input_path.stem}_processed.wav\"\n",
    "        \n",
    "        # Build FFmpeg filter chain\n",
    "        filters = [\n",
    "            'aresample=16000',  # Convert to 16kHz\n",
    "            'aformat=sample_fmts=s16:channel_layouts=mono',  # Convert to mono\n",
    "        ]\n",
    "        \n",
    "        # Add noise reduction if requested\n",
    "        if apply_noise_reduction:\n",
    "            filters.append(f'anlmdn=s={noise_reduction_strength}')\n",
    "        \n",
    "        # Add volume normalization\n",
    "        filters.append(f'volume={target_volume}')\n",
    "        \n",
    "        # Join filters into a single filter chain\n",
    "        filter_chain = ','.join(filters)\n",
    "        \n",
    "        # Build FFmpeg command\n",
    "        command = [\n",
    "            'ffmpeg',\n",
    "            '-i', str(self.input_path),\n",
    "            '-af', filter_chain,\n",
    "            '-ar', '16000',  # Ensure 16kHz output\n",
    "            '-ac', '1',      # Ensure mono output\n",
    "            '-y',            # Overwrite output if exists\n",
    "            str(output_path)\n",
    "        ]\n",
    "        \n",
    "        # Log the command for debugging\n",
    "        self.logger.info(\"Running FFmpeg command:\")\n",
    "        self.logger.info(' '.join(command))\n",
    "        \n",
    "        try:\n",
    "            # Run FFmpeg command\n",
    "            subprocess.run(command, \n",
    "                         check=True,\n",
    "                         capture_output=True,\n",
    "                         text=True)\n",
    "            \n",
    "            self.logger.info(f\"Successfully processed audio to: {output_path}\")\n",
    "            return output_path\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.logger.error(f\"FFmpeg error: {e.stderr}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during processing: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = Path(\"/Users/stephanieluo/Downloads/audio_20250811_103358_001.wav\")\n",
    "    output_dir = Path(\"/Users/stephanieluo/Downloads\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize preprocessor\n",
    "        preprocessor = AudioPreprocessor(input_file)\n",
    "        \n",
    "        # Process audio\n",
    "        processed_file = preprocessor.process_audio(\n",
    "            output_dir=output_dir,\n",
    "            apply_noise_reduction=True,\n",
    "            noise_reduction_strength=0.01,  # Adjust between 0.01 and 1.0\n",
    "            target_volume=1.5              # Adjust as needed\n",
    "        )\n",
    "        \n",
    "        print(f\"Processed file: {processed_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Processing failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363563b4-b21f-4b50-b631-9f81bfe43a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper transciption with speaker recognition\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "\n",
    "def transcribe_audio(audio_path, speaker_hints=None):\n",
    "    \"\"\"Transcribe audio using Whisper with enhanced speaker detection\"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    model = whisper.load_model(\"large-v2\", device=\"cpu\")\n",
    "    \n",
    "    # Keep the parent-infant context in the prompt but make it more transcription-focused\n",
    "    initial_prompt = \"\"\"This is a parent-infant interaction recording. \n",
    "    Please transcribe all speech, including infant vocalizations, parent speech, and any notable sounds.\n",
    "    Include both verbal and non-verbal vocalizations.\"\"\"\n",
    "    \n",
    "    print(\"Transcribing...\")\n",
    "    result = model.transcribe(\n",
    "        audio_path,\n",
    "        word_timestamps=True,\n",
    "        verbose=True,\n",
    "        initial_prompt=initial_prompt,\n",
    "        language=\"en\",\n",
    "        task=\"transcribe\"\n",
    "    )\n",
    "    \n",
    "    # Process segments\n",
    "    segments = []\n",
    "    \n",
    "    # First pass: Get clean transcriptions with timing\n",
    "    for segment in result[\"segments\"]:\n",
    "        text = segment[\"text\"].strip()\n",
    "        start = segment[\"start\"]\n",
    "        end = segment[\"end\"]\n",
    "        duration = end - start\n",
    "        \n",
    "        # Skip empty segments\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        segment_info = {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'start_time': str(timedelta(seconds=int(start))),\n",
    "            'end_time': str(timedelta(seconds=int(end))),\n",
    "            'text': text,\n",
    "            'duration': duration\n",
    "        }\n",
    "        \n",
    "        # Optional: Add speaker detection without modifying the original text\n",
    "        if speaker_hints:\n",
    "            speaker = detect_speaker(text, speaker_hints)\n",
    "            segment_info['speaker'] = speaker\n",
    "        \n",
    "        segments.append(segment_info)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(segments)\n",
    "    \n",
    "    return df, result\n",
    "\n",
    "def detect_speaker(text, speaker_hints):\n",
    "    \"\"\"Separate function for speaker detection that doesn't modify the transcription\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Basic speaker detection logic\n",
    "    if any(word in text_lower for word in ['crying', 'cries', 'waa']):\n",
    "        return '[Baby Crying]'\n",
    "    elif any(word in text_lower for word in ['laugh', 'giggle']):\n",
    "        return '[Baby Laughing]'\n",
    "    elif any(word in text_lower for word in ['goo', 'gah', 'bah', 'coo']):\n",
    "        return '[Infant Vocalization]'\n",
    "    elif '[mother]' in text_lower or 'mom' in text_lower:\n",
    "        return '[Mother]'\n",
    "    elif '[father]' in text_lower or 'dad' in text_lower:\n",
    "        return '[Father]'\n",
    "    else:\n",
    "        return '[Unspecified]'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    speaker_hints = [\n",
    "        \"[Mother]\", \"[Father]\", \"[Baby]\",\n",
    "        \"[Infant Vocalization]\", \"[Baby Crying]\", \"[Baby Laughing]\"\n",
    "    ]\n",
    "\n",
    "    ###replace with your audio file\n",
    "    try:\n",
    "        df, result = transcribe_audio(\n",
    "            \"/Users/yueyan/Documents/project/wearable/media/12mon/your file name\",\n",
    "            speaker_hints=speaker_hints\n",
    "        )\n",
    "        \n",
    "        # Save results; replace with your path\n",
    "        df.to_csv(\"/Users/yueyan/Documents/project/wearable/transcription/12mon/your file name to be saved\", index=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nTranscription Statistics:\")\n",
    "        print(f\"Total segments: {len(df)}\")\n",
    "        \n",
    "        # If speaker detection is enabled\n",
    "        if 'speaker' in df.columns:\n",
    "            print(\"\\nSpeaker distribution:\")\n",
    "            print(df['speaker'].value_counts())\n",
    "        \n",
    "        print(\"\\nFirst few transcriptions:\")\n",
    "        print(df[['start_time', 'end_time', 'text']].head())\n",
    "        \n",
    "        # Optionally display with speakers if available\n",
    "        if 'speaker' in df.columns:\n",
    "            print(\"\\nFirst few transcriptions with speakers:\")\n",
    "            print(df[['start_time', 'end_time', 'speaker', 'text']].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d8961-e6c8-469d-ba0b-697ff11785eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 18:55:41,485 - INFO - Note: NumExpr detected 11 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2025-09-04 18:55:41,485 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:07.820 --> 00:09.340]  Do you think my phone is around there?\n",
      "[00:09.440 --> 00:09.640]  No.\n",
      "[00:09.860 --> 00:12.080]  I think I heard it run there.\n",
      "[00:15.860 --> 00:17.660]  Do you want me to call you?\n",
      "[00:18.220 --> 00:20.080]  Yeah, I know I heard it but I was wondering.\n",
      "[00:22.680 --> 00:25.820]  I think it is back on.\n",
      "[00:34.860 --> 00:39.500]  little bunny poop poop hop into the forest\n",
      "[00:40.640 --> 00:41.600]  wow\n",
      "[00:44.020 --> 00:44.980]  wow\n",
      "[00:51.500 --> 00:52.460]  wow\n",
      "[00:52.460 --> 00:53.180]  wow\n",
      "[00:53.180 --> 00:54.460]  wow\n",
      "[00:54.460 --> 00:54.760]  wow\n",
      "[00:54.760 --> 00:54.900]  wow\n",
      "[00:54.900 --> 00:55.160]  wow\n",
      "[00:55.160 --> 00:55.220]  wow\n",
      "[00:55.220 --> 00:55.520]  wow\n",
      "[00:55.520 --> 00:55.780]  wow\n",
      "[00:55.780 --> 00:55.800]  wow\n",
      "[00:55.800 --> 00:55.800]  wow\n",
      "[00:55.800 --> 00:55.800]  wow\n",
      "[00:55.800 --> 00:55.800]  wow\n",
      "[00:55.800 --> 00:55.800]  wow\n",
      "[01:09.640 --> 01:11.040]  wow\n",
      "[01:24.380 --> 01:25.780]  for you\n",
      "[01:25.780 --> 01:26.900]  It's a bunny!\n",
      "[01:27.660 --> 01:30.040]  Oh! The bunny's here!\n",
      "[01:30.240 --> 01:31.140]  Where's your ear?\n",
      "[01:31.900 --> 01:33.440]  No, no te lo quites, KJ!\n",
      "[01:33.740 --> 01:35.360]  No te lo quites, no thank you!\n",
      "[01:35.640 --> 01:36.440]  Don't take it off!\n",
      "[01:37.020 --> 01:40.380]  Oh, KJ, I know! I know it's a sting, right?\n",
      "[01:40.620 --> 01:40.960]  But see?\n",
      "[01:41.640 --> 01:43.020]  Bunny also has a sting.\n",
      "[01:43.580 --> 01:43.980]  I know!\n",
      "[01:44.160 --> 01:46.100]  I can probably take out more toys over here.\n",
      "[01:46.380 --> 01:48.620]  Yes, yes, we can do that.\n",
      "[01:49.080 --> 01:51.660]  Yeah, I know, you can feel that, right?\n",
      "[01:52.420 --> 01:53.620]  Do you want to hold it?\n",
      "[01:53.620 --> 01:55.260]  Do you want to hold it?\n",
      "[01:56.000 --> 01:56.800]  I know, I know!\n",
      "[01:56.980 --> 01:57.860]  Do you want to hold it?\n",
      "[01:59.560 --> 02:00.780]  So many toys!\n",
      "[02:02.060 --> 02:02.120]  Wow!\n",
      "[02:02.580 --> 02:03.520]  Wow, scuba!\n",
      "[02:04.840 --> 02:05.960]  Do you like scuba?\n",
      "[02:07.040 --> 02:08.740]  If you want, I can put him on the floor.\n",
      "[02:09.120 --> 02:09.580]  Oh, okay.\n",
      "[02:11.760 --> 02:14.720]  I can bring out the mat if you guys want to sit on it.\n",
      "[02:15.240 --> 02:17.420]  We just want to make sure he's comfortable.\n",
      "[02:19.520 --> 02:20.720]  I know, right?\n",
      "[02:21.000 --> 02:22.360]  Om, om, om!\n",
      "[02:22.800 --> 02:23.600]  You're eating the toy?\n",
      "[02:23.600 --> 02:25.120]  You're eating the Velcro.\n",
      "[02:25.980 --> 02:28.460]  Om, om, om, om!\n",
      "[02:28.520 --> 02:29.720]  Our house flooded.\n",
      "[02:30.340 --> 02:31.060]  A bunch of blood.\n",
      "[02:31.340 --> 02:32.880]  Our AC started leaking up.\n",
      "[02:33.160 --> 02:36.260]  So we're in the middle of\n",
      "[02:36.260 --> 02:37.600]  opening up\n",
      "[02:38.640 --> 02:39.840]  an insurance claim.\n",
      "[02:39.920 --> 02:41.740]  And we're going to have to get the bathroom redone.\n",
      "[02:42.240 --> 02:43.600]  So we're leaking everything.\n",
      "[02:43.780 --> 02:45.260]  We're going to pass out the whole house.\n",
      "[02:45.560 --> 02:47.860]  Oh, it's a busy time.\n",
      "[02:48.240 --> 02:49.120]  I know, right?\n",
      "[02:49.400 --> 02:50.560]  He took the hat off.\n",
      "[02:51.660 --> 02:52.380]  That's fun.\n",
      "[02:52.600 --> 02:54.360]  Do you want to come here?\n",
      "[02:54.880 --> 02:55.240]  And play?\n",
      "[02:57.380 --> 02:57.900]  Yeah?\n",
      "[02:59.080 --> 03:00.120]  No, don't take it off.\n",
      "[03:00.660 --> 03:01.420]  Don't take it off.\n",
      "[03:02.240 --> 03:02.600]  Oh!\n",
      "[03:05.780 --> 03:06.520]  Come here, come here.\n",
      "[03:06.580 --> 03:07.100]  Let's go play.\n",
      "[03:07.360 --> 03:08.160]  Look, look.\n",
      "[03:10.080 --> 03:11.320]  Bring the toy.\n",
      "[03:12.620 --> 03:13.900]  No, don't take it off.\n",
      "[03:15.000 --> 03:15.960]  Look, look.\n",
      "[03:17.160 --> 03:17.320]  Look.\n",
      "[03:18.440 --> 03:19.000]  Wow!\n",
      "[03:19.920 --> 03:20.480]  KJ!\n",
      "[03:21.240 --> 03:21.740]  Superman!\n",
      "[03:24.100 --> 03:25.040]  What is this?\n",
      "[03:25.960 --> 03:26.780]  What is this?\n",
      "[03:27.640 --> 03:28.580]  And what do we have?\n",
      "[03:29.160 --> 03:30.700]  No, don't take it off.\n",
      "[03:30.980 --> 03:32.080]  Put it on.\n",
      "[03:33.620 --> 03:35.720]  I know this is not comfortable, right?\n",
      "[03:35.720 --> 03:36.140]  Peek-a-boo!\n",
      "[03:36.420 --> 03:37.600]  Peek-a-boo!\n",
      "[03:37.820 --> 03:38.440]  Peek-a-boo!\n",
      "[03:42.060 --> 03:42.780]  Say it.\n",
      "[03:44.440 --> 03:45.260]  Where's KJ?\n",
      "[03:46.240 --> 03:47.300]  Where's KJ?\n",
      "[03:51.720 --> 03:52.740]  Where's KJ?\n",
      "[03:53.800 --> 03:55.440]  Say, peek-a-boo.\n",
      "[03:57.220 --> 03:59.040]  I see you!\n",
      "[04:01.880 --> 04:02.700]  Come here.\n",
      "[04:02.960 --> 04:03.380]  Peek-a-boo.\n",
      "[04:05.260 --> 04:05.940]  Peek-a-boo!\n",
      "[04:07.800 --> 04:09.820]  Oh, you like this, huh?\n",
      "[04:10.440 --> 04:10.920]  You like it?\n",
      "[04:11.280 --> 04:11.900]  Uh-uh.\n",
      "[04:15.260 --> 04:15.780]  Where's KJ?\n",
      "[04:16.700 --> 04:17.140]  Peek-a-boo!\n",
      "[04:19.880 --> 04:20.460]  Where's mom?\n",
      "[04:20.980 --> 04:21.360]  Where's mom?\n",
      "[04:21.820 --> 04:21.920]  Oh!\n",
      "[04:25.740 --> 04:26.400]  Peek-a-boo!\n",
      "[04:28.200 --> 04:29.120]  Peek-a-boo!\n",
      "[04:29.420 --> 04:29.800]  Like that?\n",
      "[04:32.500 --> 04:33.860]  The camera is not in the middle\n",
      "[04:33.860 --> 04:34.820]  because the way it pulls,\n",
      "[04:35.100 --> 04:36.780]  it has to be aligned differently.\n",
      "[04:37.320 --> 04:38.460]  Is this the first one you guys do?\n",
      "[04:41.940 --> 04:42.720]  Oh, really?\n",
      "[04:42.720 --> 04:44.640]  You can raise it or unfold it\n",
      "[04:44.640 --> 04:46.300]  so it's like here.\n",
      "[04:46.540 --> 04:48.180]  The align one is a little bit off.\n",
      "[04:49.120 --> 04:49.620]  Yeah, oh!\n",
      "[04:50.240 --> 04:52.280]  It looks so cute, KJ.\n",
      "[04:52.380 --> 04:53.920]  Yes, I don't think you can center it.\n",
      "[04:55.940 --> 04:56.860]  Peek-a-boo!\n",
      "[04:57.680 --> 04:59.420]  It feels like it's centered, though, no?\n",
      "[04:59.980 --> 05:01.240]  It feels like it's centered, though, no?\n",
      "[05:01.240 --> 05:02.520]  Here, we can move it a half a little bit\n",
      "[05:02.520 --> 05:03.200]  to the right.\n",
      "[05:03.460 --> 05:05.820]  And we're the right...\n",
      "[05:07.500 --> 05:08.640]  That's not the middle.\n",
      "[05:09.040 --> 05:11.200]  That's the blue line.\n",
      "[05:11.200 --> 05:15.780]  Sorry, the blue line is just the...\n",
      "[05:15.780 --> 05:17.200]  Oh, you want it...\n",
      "[05:17.200 --> 05:18.120]  Oh, gotcha, gotcha.\n",
      "[05:18.340 --> 05:19.180]  That's why I can't...\n",
      "[05:19.180 --> 05:20.060]  Ben! Ben!\n",
      "[05:20.500 --> 05:20.960]  Ben!\n",
      "[05:21.880 --> 05:22.420]  Ben!\n",
      "[05:22.820 --> 05:27.500]  Ben, let me fix it.\n",
      "[05:27.660 --> 05:28.640]  Let me fix it a bit, okay, okay.\n",
      "[05:29.060 --> 05:29.540]  Like this?\n",
      "[05:31.220 --> 05:31.760]  Yay!\n",
      "[05:33.900 --> 05:34.440]  Look at KJ!\n",
      "[05:37.160 --> 05:37.700]  Wow!\n",
      "[05:38.780 --> 05:39.320]  Wow!\n",
      "[05:39.320 --> 05:40.040]  Wow!\n",
      "[05:42.700 --> 05:43.060]  Yeah?\n",
      "[05:43.660 --> 05:43.900]  Yeah?\n",
      "[05:45.320 --> 05:45.960]  Let's just do this.\n",
      "[05:47.440 --> 05:48.160]  Yeah!\n",
      "[05:49.400 --> 05:50.140]  Good job!\n",
      "[05:50.680 --> 05:51.040]  Bravo!\n",
      "[05:53.500 --> 05:54.360]  You did so well!\n",
      "[05:56.020 --> 05:56.300]  Ah!\n",
      "[05:57.140 --> 05:57.440]  Yeah?\n",
      "[06:00.760 --> 06:01.620]  Ba-ba!\n",
      "[06:02.320 --> 06:03.480]  Do you think once we...\n",
      "[06:03.480 --> 06:04.300]  He will notice?\n",
      "[06:04.980 --> 06:05.700]  Yeah?\n",
      "[06:05.700 --> 06:06.220]  Ah!\n",
      "[06:07.100 --> 06:08.580]  Or we can hide it.\n",
      "[06:09.400 --> 06:10.000]  Okay.\n",
      "[06:10.540 --> 06:11.860]  Yeah, yeah, yeah, good, good.\n",
      "[06:12.020 --> 06:15.280]  And we'll hide it outside.\n",
      "[06:16.680 --> 06:16.680]  Okay.\n",
      "[06:18.020 --> 06:19.060]  I'll...\n",
      "[06:19.060 --> 06:19.440]  Okay.\n",
      "[06:21.780 --> 06:23.260]  Bravo, KJ!\n",
      "[06:27.400 --> 06:28.440]  Okay.\n",
      "[06:29.280 --> 06:30.000]  Wow!\n",
      "[06:31.220 --> 06:31.500]  Wow!\n",
      "[06:31.500 --> 06:31.500]  Ah!\n",
      "[06:32.160 --> 06:32.680]  Ah!\n",
      "[06:33.500 --> 06:38.080]  Okay, okay.\n",
      "[06:38.740 --> 06:39.300]  No!\n",
      "[06:40.520 --> 06:41.580]  Thank you!\n",
      "[06:43.120 --> 06:44.720]  No, thank you.\n",
      "[06:44.760 --> 06:46.000]  Okay, got it, thank you.\n",
      "[06:46.440 --> 06:48.240]  Let us know if you need help.\n",
      "[06:48.540 --> 06:50.480]  No, no, thank you!\n",
      "[06:50.840 --> 06:51.760]  No, thank you!\n",
      "[06:52.560 --> 06:53.560]  No, thank you!\n",
      "[06:54.400 --> 06:54.960]  Bravo!\n",
      "[06:57.960 --> 06:58.640]  Hey, Ben!\n",
      "[07:00.260 --> 07:01.280]  Sometimes, yeah,\n",
      "[07:01.480 --> 07:03.400]  it might be a little bit difficult.\n",
      "[07:03.520 --> 07:04.340]  Adjusting? Okay.\n",
      "[07:05.680 --> 07:09.040]  But we also want to make him comfortable and happy.\n",
      "[07:09.320 --> 07:11.700]  So if he doesn't like it or cry,\n",
      "[07:12.960 --> 07:14.500]  you'll have to put it on.\n",
      "[07:14.720 --> 07:15.580]  Okay, okay.\n",
      "[07:15.980 --> 07:16.580]  Let us know.\n",
      "[07:16.840 --> 07:17.400]  Bravo!\n",
      "[07:17.880 --> 07:19.060]  Thank you.\n",
      "[07:19.240 --> 07:19.540]  Thank you.\n",
      "[07:19.980 --> 07:21.740]  Dile, I'll see you guys in a little bit.\n",
      "[07:22.580 --> 07:23.600]  Dile bye!\n",
      "[07:23.980 --> 07:24.460]  Dile bye!\n",
      "[07:24.460 --> 07:25.360]  Dile, ahorita vienen.\n",
      "[07:37.360 --> 07:39.460]  You'll come back in 30?\n",
      "[07:40.000 --> 07:40.380]  Yeah.\n",
      "[07:41.360 --> 07:41.760]  Yeah.\n",
      "[07:44.380 --> 07:44.540]  Okay.\n",
      "[07:48.640 --> 07:49.480]  Mite, mite.\n",
      "[07:50.360 --> 07:50.780]  Ya.\n",
      "[07:53.300 --> 07:55.240]  Wow, PJ!\n",
      "[07:57.200 --> 07:58.080]  Wow!\n",
      "[07:58.740 --> 07:59.480]  Look at the scissors.\n",
      "[08:02.000 --> 08:02.880]  Wow!\n",
      "[08:05.040 --> 08:06.940]  Mira, ontas las pelotas.\n",
      "[08:08.620 --> 08:08.840]  Mira.\n",
      "[08:10.460 --> 08:12.220]  Cinco, seis.\n",
      "[08:13.080 --> 08:14.400]  Ben, mira, aqui hay mas.\n",
      "[08:16.420 --> 08:17.500]  Aqui hay mas.\n",
      "[08:18.760 --> 08:20.320]  Aqui hay mas.\n",
      "[08:20.660 --> 08:20.760]  Mira, ven.\n",
      "[08:20.760 --> 08:20.760]  Ven, ven.\n",
      "[08:20.760 --> 08:20.760]  Ven, ven.\n",
      "[08:20.760 --> 08:21.520]  Mira, aqui.\n",
      "[08:28.760 --> 08:30.120]  Mira, es.\n",
      "[08:31.520 --> 08:32.360]  Es.\n",
      "[08:33.900 --> 08:34.300]  Mira.\n",
      "[08:35.300 --> 08:35.640]  G.\n",
      "[08:35.860 --> 08:37.800]  What is it? What color is it?\n",
      "[08:38.000 --> 08:39.520]  PJ, PJ.\n",
      "[08:41.180 --> 08:42.020]  Pink.\n",
      "[08:43.040 --> 08:43.300]  Pink.\n",
      "[08:55.720 --> 08:56.780]  Pfft.\n",
      "[09:03.340 --> 09:05.460]  Look it.\n",
      "[09:06.020 --> 09:07.520]  Look it.\n",
      "[09:08.760 --> 09:08.780]  Wow!\n",
      "[09:10.160 --> 09:10.560]  Uh-oh.\n",
      "[09:10.860 --> 09:11.820]  Uh-oh.\n",
      "[09:14.880 --> 09:17.280]  What happened? Look.\n",
      "[09:17.280 --> 09:19.120]  Blocks. The blocks.\n",
      "[09:24.500 --> 09:27.240]  Why are we throwing them? What's the matter?\n",
      "[09:27.720 --> 09:28.000]  Look it.\n",
      "[09:28.280 --> 09:28.920]  One.\n",
      "[09:30.460 --> 09:31.100]  Two.\n",
      "[09:33.040 --> 09:33.920]  Where's the other one?\n",
      "[09:35.480 --> 09:35.740]  Look.\n",
      "[09:36.080 --> 09:37.140]  Look at the puzzle.\n",
      "[09:38.260 --> 09:39.300]  Look at the puzzle.\n",
      "[09:39.700 --> 09:40.340]  Green.\n",
      "[09:43.040 --> 09:43.680]  Pink.\n",
      "[09:43.680 --> 09:43.960]  Pink.\n",
      "[09:45.680 --> 09:45.860]  Yellow.\n",
      "[09:47.220 --> 09:47.980]  No, you don't want the color?\n",
      "[09:48.860 --> 09:48.860]  No.\n",
      "[09:57.400 --> 09:58.260]  Wow!\n",
      "[10:01.680 --> 10:01.960]  Wow!\n",
      "[10:05.680 --> 10:05.980]  Look it.\n",
      "[10:06.780 --> 10:07.220]  Look it.\n",
      "[10:09.760 --> 10:10.440]  Look it.\n",
      "[10:10.440 --> 10:11.420]  Haha.\n",
      "[10:12.740 --> 10:14.140]  Haha.\n",
      "[10:18.440 --> 10:19.260]  Ten.\n",
      "[10:20.680 --> 10:20.940]  Ten?\n",
      "[10:22.380 --> 10:23.780]  Wow!\n",
      "[10:28.000 --> 10:30.000]  Look it.\n",
      "[10:45.500 --> 10:46.100]  Look it.\n",
      "[10:46.100 --> 10:46.100]  Look it.\n",
      "[10:46.100 --> 10:46.100]  Look it.\n",
      "[10:46.100 --> 10:46.780]  No, no, no.\n",
      "[10:48.220 --> 10:49.700]  No, no, no.\n",
      "[10:50.240 --> 10:51.340]  No, leave it.\n",
      "[10:51.540 --> 10:51.980]  Leave it.\n",
      "[10:53.160 --> 10:53.760]  Wow!\n",
      "[10:56.320 --> 10:57.580]  Wow, PJ.\n",
      "[10:57.720 --> 10:58.020]  Look it.\n",
      "[10:58.420 --> 11:02.440]  The wheels and the...\n",
      "[11:10.660 --> 11:12.060]  Oops.\n",
      "[11:14.660 --> 11:15.560]  No.\n",
      "[11:20.060 --> 11:21.460]  Wow!\n",
      "[11:29.580 --> 11:30.980]  Wow!\n",
      "[11:43.060 --> 11:43.940]  Woo.\n",
      "[11:44.340 --> 11:45.160]  Round and round.\n",
      "[11:45.740 --> 11:52.120]  Round and round.\n",
      "[11:52.220 --> 11:53.700]  Round and round.\n",
      "[11:53.840 --> 11:55.020]  The wheels and the...\n",
      "[11:55.020 --> 12:00.540]  All through the...\n",
      "[12:00.540 --> 12:00.980]  Town.\n",
      "[12:23.680 --> 12:26.480]  Call you later.\n",
      "[12:26.480 --> 12:27.200]  What's up?\n",
      "[12:28.060 --> 12:29.520]  I'm going to the bathroom.\n",
      "[12:30.140 --> 12:32.240]  If you're going to the bathroom, why are you talking to me?\n",
      "[12:34.140 --> 12:35.820]  No, KJ. No.\n",
      "[12:36.660 --> 12:38.480]  No, KJ. No.\n",
      "[12:39.080 --> 12:40.560]  No, thank you.\n",
      "[12:41.120 --> 12:41.440]  Wow!\n",
      "[12:42.820 --> 12:43.940]  Kiss, kiss, mama.\n",
      "[12:44.100 --> 12:44.980]  Kiss, mama.\n",
      "[12:45.780 --> 12:46.360]  Mwah!\n",
      "[12:48.240 --> 12:48.960]  Mmmm.\n",
      "[12:50.060 --> 12:50.620]  Kiss, mama.\n",
      "[12:52.220 --> 12:52.940]  Mwah!\n",
      "[12:52.940 --> 12:52.940]  Mwah!\n",
      "[12:55.300 --> 12:55.840]  Peek-a-boo!\n",
      "[12:56.500 --> 12:57.900]  No, no.\n",
      "[12:58.740 --> 12:59.900]  No, take it off.\n",
      "[13:00.940 --> 13:02.660]  No, no.\n",
      "[13:03.120 --> 13:04.100]  Come, come.\n",
      "[13:04.220 --> 13:04.900]  Come, come.\n",
      "[13:05.700 --> 13:06.480]  Vroom!\n",
      "[13:09.760 --> 13:10.540]  Vroom!\n",
      "[13:12.900 --> 13:12.940]  Vroom!\n",
      "[13:17.360 --> 13:18.140]  Vroom!\n",
      "[13:18.520 --> 13:18.780]  Vroom!\n",
      "[13:19.900 --> 13:20.000]  Vroom!\n",
      "[13:20.600 --> 13:21.080]  Vroom!\n",
      "[13:22.940 --> 13:23.440]  Vroom!\n",
      "[13:24.940 --> 13:25.000]  Vroom!\n",
      "[13:33.300 --> 13:34.380]  Beep-beep!\n",
      "[13:34.760 --> 13:35.640]  Beep-beep!\n",
      "[13:37.440 --> 13:38.440]  Beep-beep!\n",
      "[13:41.880 --> 13:43.060]  Alexa, play\n",
      "[13:43.060 --> 13:44.160]  the wheels on the bus.\n",
      "[14:00.380 --> 14:01.780]  Yeah.\n",
      "[14:01.780 --> 14:01.780]  Yeah.\n",
      "[14:11.720 --> 14:13.120]  Wheeee!\n",
      "[14:16.940 --> 14:17.880]  Peek-a-boo!\n",
      "[14:17.880 --> 14:19.040]  Peek-a-boo!\n",
      "[14:21.880 --> 14:22.280]  Peek-a-boo!\n",
      "[14:26.700 --> 14:27.740]  Where's KJ?\n",
      "[14:29.080 --> 14:30.160]  You want to see in the box?\n",
      "[14:32.680 --> 14:33.800]  What's inside?\n",
      "[14:34.740 --> 14:36.460]  What's inside the box?\n",
      "[14:37.980 --> 14:39.320]  There's nothing.\n",
      "[14:42.160 --> 14:43.200]  There's nothing.\n",
      "[14:43.280 --> 14:44.400]  Put something inside the box.\n"
     ]
    }
   ],
   "source": [
    "##whisper transcription without speaker recognition\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"Transcribe audio using Whisper\"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    model = whisper.load_model(\"large-v2\", device=\"cpu\")\n",
    "    \n",
    "    # Keep the parent-infant context in the prompt but make it more transcription-focused\n",
    "    initial_prompt = \"\"\"This is a parent-infant interaction recording. \"\"\"\n",
    "#    Please transcribe all speech, including infant vocalizations, parent speech, and any notable sounds.\n",
    "#    Include both verbal and non-verbal vocalizations.\"\"\"\n",
    "    \n",
    "    print(\"Transcribing...\")\n",
    "    result = model.transcribe(\n",
    "        audio_path,\n",
    "        word_timestamps=True,\n",
    "        verbose=True,\n",
    "        initial_prompt=initial_prompt,\n",
    "        language=\"en\",\n",
    "        task=\"transcribe\"\n",
    "    )\n",
    "    \n",
    "    # Process segments\n",
    "    segments = []\n",
    "    \n",
    "    # First pass: Get clean transcriptions with timing\n",
    "    for segment in result[\"segments\"]:\n",
    "        text = segment[\"text\"].strip()\n",
    "        start = segment[\"start\"]\n",
    "        end = segment[\"end\"]\n",
    "        duration = end - start\n",
    "        \n",
    "        # Skip empty segments\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        segment_info = {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'start_time': str(timedelta(seconds=int(start))),\n",
    "            'end_time': str(timedelta(seconds=int(end))),\n",
    "            'text': text,\n",
    "            'duration': duration\n",
    "        }\n",
    "        \n",
    "        segments.append(segment_info)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(segments)\n",
    "    \n",
    "    return df, result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df, result = transcribe_audio(\n",
    "            \"/Users/stephanieluo/Downloads/audio_20250811_103358_001_processed.wav\"\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        df.to_csv(\"/Users/stephanieluo/Downloads/20250811_103358_001_whisper_LL.csv\", index=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nTranscription Statistics:\")\n",
    "        print(f\"Total segments: {len(df)}\")\n",
    "        \n",
    "        print(\"\\nFirst few transcriptions:\")\n",
    "        print(df[['start_time', 'end_time', 'text']].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c86f1685-fc99-455a-88ed-239537ee6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "###load datasets\n",
    "whisper_df = pd.read_csv(\"/Users/yueyan/Documents/project/wearable/transcription/whisper/12mon/IW_038_12_whisper_wopre_wos_111924.csv\")\n",
    "human_df = pd.read_csv(\"/Users/yueyan/Documents/project/wearable/transcription/human/12mon/IW_038_12_human_transcription.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be6b912-4d90-4314-b6c1-cfe298211fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing segments: 180it [00:00, 3118.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison Summary:\n",
      "Total segments analyzed: 121\n",
      "Average accuracy: 74.2%\n",
      "Average similarity score: 0.806\n",
      "High quality matches (>80%): 72\n",
      "Low quality matches (<60%): 39\n",
      "\n",
      "Sample comparison results:\n",
      "  Time Range  Human Transcription  \\\n",
      "0  37.2-38.4          oh squeaky.   \n",
      "1  46.5-47.9    oh did it tickle?   \n",
      "2  46.5-47.9    oh did it tickle?   \n",
      "3  48.4-49.5  did it tickle here.   \n",
      "4  52.1-52.3             come on.   \n",
      "\n",
      "                          Whisper Transcription  Accuracy (%)  \n",
      "0                                  Oh, squeaky!         100.0  \n",
      "1                            Oh, did it tickle?         100.0  \n",
      "2  Did it tickle? Here, got it? Oh, you got it!         100.0  \n",
      "3  Did it tickle? Here, got it? Oh, you got it!         100.0  \n",
      "4                                      Come on.         100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "class EnhancedTranscriptionComparator:\n",
    "    def __init__(self, whisper_df: pd.DataFrame, human_df: pd.DataFrame):\n",
    "        \"\"\"Initialize with enhanced text cleaning and comparison capabilities\"\"\"\n",
    "        self.whisper_data = whisper_df.copy()\n",
    "        self.human_data = human_df.copy()\n",
    "        \n",
    "        # Pre-clean the data\n",
    "        self.human_data = self.human_data[self.human_data['text'].notna()]\n",
    "        if 'end ' in self.human_data.columns:\n",
    "            self.human_data = self.human_data.rename(columns={'end ': 'end'})\n",
    "        \n",
    "        # Convert times to numeric\n",
    "        self.human_data['start'] = pd.to_numeric(self.human_data['start'])\n",
    "        self.human_data['end'] = pd.to_numeric(self.human_data['end'])\n",
    "        \n",
    "        # Clean text and store originals\n",
    "        self.whisper_data['clean_text'] = self.whisper_data['text'].apply(self.clean_text)\n",
    "        self.human_data['clean_text'] = self.human_data['text'].apply(self.clean_text)\n",
    "        self.whisper_data['original_text'] = self.whisper_data['text']\n",
    "        self.human_data['original_text'] = self.human_data['text']\n",
    "        \n",
    "        # Find overlap range\n",
    "        self.overlap_start = max(self.whisper_data['start'].min(), self.human_data['start'].min())\n",
    "        self.overlap_end = min(self.whisper_data['end'].max(), self.human_data['end'].max())\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"Enhanced text cleaning function\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "            \n",
    "        text = str(text).lower()\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        replacements = {\n",
    "            'uhm': 'um', 'uhh': 'uh', 'hmm': 'hm',\n",
    "            'mhm': 'mm', 'yeah': 'yes', 'yah': 'yes',\n",
    "            'nah': 'no'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = re.sub(r'\\b' + old + r'\\b', new, text)\n",
    "            \n",
    "        return text\n",
    "\n",
    "    def get_word_metrics(self, human_text: str, whisper_text: str) -> Dict[str, int]:\n",
    "        \"\"\"Calculate detailed word-level metrics\"\"\"\n",
    "        human_words = set(self.clean_text(human_text).split())\n",
    "        whisper_words = set(self.clean_text(whisper_text).split())\n",
    "        \n",
    "        correct_words = len(human_words.intersection(whisper_words))\n",
    "        total_words_human = len(human_words)\n",
    "        total_words_whisper = len(whisper_words)\n",
    "        mismatches = max(total_words_human, total_words_whisper) - correct_words\n",
    "        \n",
    "        return {\n",
    "            'word_count_human': total_words_human,\n",
    "            'word_count_whisper': total_words_whisper,\n",
    "            'correct_words': correct_words,\n",
    "            'mismatches': mismatches\n",
    "        }\n",
    "\n",
    "    def generate_comparison_results(self, time_tolerance: float = 0.5, \n",
    "                                  similarity_threshold: float = 0.3) -> pd.DataFrame:\n",
    "        \"\"\"Generate comprehensive comparison results for spreadsheet\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        human_segments = self.human_data[\n",
    "            (self.human_data['start'] >= self.overlap_start) &\n",
    "            (self.human_data['end'] <= self.overlap_end)\n",
    "        ]\n",
    "        \n",
    "        whisper_segments = self.whisper_data[\n",
    "            (self.whisper_data['start'] >= self.overlap_start) &\n",
    "            (self.whisper_data['end'] <= self.overlap_end)\n",
    "        ]\n",
    "        \n",
    "        for _, human_seg in tqdm(human_segments.iterrows(), desc=\"Analyzing segments\"):\n",
    "            potential_matches = whisper_segments[\n",
    "                (whisper_segments['start'] >= human_seg['start'] - time_tolerance) &\n",
    "                (whisper_segments['start'] <= human_seg['end'] + time_tolerance)\n",
    "            ]\n",
    "            \n",
    "            for _, whisper_seg in potential_matches.iterrows():\n",
    "                similarity = SequenceMatcher(\n",
    "                    None,\n",
    "                    human_seg['clean_text'],\n",
    "                    whisper_seg['clean_text']\n",
    "                ).ratio()\n",
    "                \n",
    "                if similarity > similarity_threshold:\n",
    "                    # Get word-level metrics\n",
    "                    word_metrics = self.get_word_metrics(\n",
    "                        human_seg['original_text'],\n",
    "                        whisper_seg['original_text']\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    accuracy = (word_metrics['correct_words'] / word_metrics['word_count_human'] * 100) \\\n",
    "                        if word_metrics['word_count_human'] > 0 else 0\n",
    "                    \n",
    "                    # Generate comments\n",
    "                    comments = []\n",
    "                    if similarity < 0.5:\n",
    "                        comments.append(\"Low similarity score\")\n",
    "                    if abs(word_metrics['word_count_human'] - word_metrics['word_count_whisper']) > 3:\n",
    "                        comments.append(\"Significant word count difference\")\n",
    "                    if accuracy < 60:\n",
    "                        comments.append(\"Low accuracy\")\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Time Range': f\"{human_seg['start']:.1f}-{human_seg['end']:.1f}\",\n",
    "                        'Whisper Transcription': whisper_seg['original_text'],\n",
    "                        'Human Transcription': human_seg['original_text'],\n",
    "                        'SequenceMatcher Score': round(similarity, 3),\n",
    "                        'Word Count (Human)': word_metrics['word_count_human'],\n",
    "                        'Word Count (Whisper)': word_metrics['word_count_whisper'],\n",
    "                        'Correct Words': word_metrics['correct_words'],\n",
    "                        'Mismatches': word_metrics['mismatches'],\n",
    "                        'Accuracy (%)': round(accuracy, 1),\n",
    "                        'Comments/Notes': \"; \".join(comments) if comments else \"OK\"\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "def compare_and_export(whisper_file: pd.DataFrame, human_file: pd.DataFrame, \n",
    "                      output_path: str = \"transcription_comparison.csv\") -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"Compare transcriptions and export results\"\"\"\n",
    "    # Initialize comparator\n",
    "    comparator = EnhancedTranscriptionComparator(whisper_file, human_file)\n",
    "    \n",
    "    # Generate comparison results\n",
    "    results_df = comparator.generate_comparison_results()\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    stats = {\n",
    "        'Total Segments': len(results_df),\n",
    "        'Average Accuracy': results_df['Accuracy (%)'].mean(),\n",
    "        'Average Similarity': results_df['SequenceMatcher Score'].mean(),\n",
    "        'High Quality Matches': len(results_df[results_df['Accuracy (%)'] > 80]),\n",
    "        'Low Quality Matches': len(results_df[results_df['Accuracy (%)'] < 60])\n",
    "    }\n",
    "    \n",
    "    # Export to CSV\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nComparison Summary:\")\n",
    "    print(f\"Total segments analyzed: {stats['Total Segments']}\")\n",
    "    print(f\"Average accuracy: {stats['Average Accuracy']:.1f}%\")\n",
    "    print(f\"Average similarity score: {stats['Average Similarity']:.3f}\")\n",
    "    print(f\"High quality matches (>80%): {stats['High Quality Matches']}\")\n",
    "    print(f\"Low quality matches (<60%): {stats['Low Quality Matches']}\")\n",
    "    \n",
    "    return results_df, stats\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #whisper_df and human_df are the input DataFrames\n",
    "    results_df, stats = compare_and_export(whisper_df, human_df, \"/Users/yueyan/Documents/project/wearable/transcription/comparison/12mon/IW_038_12_transcription_comparison.csv\")\n",
    "    \n",
    "    # Display first few results\n",
    "    print(\"\\nSample comparison results:\")\n",
    "    print(results_df[['Time Range', 'Human Transcription', \n",
    "                     'Whisper Transcription', 'Accuracy (%)']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109b288-71ba-4f5e-b7e0-a2df248ba215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
