{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe8126-3f4d-44d5-b25c-cc5dba10313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from datasets import Audio, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "\n",
    "# Use relative paths to work with SLURM environments\n",
    "BASE_DIR = \"/home/yyt005/whisper_fine_tuning\"  \n",
    "PROCESSED_AUDIO_PATH = os.path.join(BASE_DIR, \"processed_audio\")\n",
    "MODEL_OUTPUT_PATH = os.path.join(BASE_DIR, \"fine-tuned-whisper-large\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "meta_df = pd.read_csv(os.path.join(BASE_DIR, 'metadata.csv'))\n",
    "\n",
    "# Process audio file paths\n",
    "def update_filepath(path):\n",
    "    # Extract the filename and its parent directory\n",
    "    filename = os.path.basename(path)\n",
    "    subdirectory = os.path.basename(os.path.dirname(path))\n",
    "    return os.path.join(PROCESSED_AUDIO_PATH, subdirectory, filename)\n",
    "\n",
    "meta_df['audio_filepath'] = meta_df['audio_filepath'].apply(update_filepath)\n",
    "\n",
    "# Check if files exist\n",
    "valid_files = []\n",
    "for idx, row in meta_df.iterrows():\n",
    "    if os.path.exists(row['audio_filepath']):\n",
    "        valid_files.append(idx)\n",
    "    else:\n",
    "        print(f\"Warning: File not found - {row['audio_filepath']}\")\n",
    "\n",
    "meta_df = meta_df.loc[valid_files]\n",
    "print(f\"Processing {len(meta_df)} valid audio files\")\n",
    "\n",
    "# Create train/test splits\n",
    "train_sample = meta_df.sample(n=min(100, len(meta_df)//2))\n",
    "test_sample = meta_df.drop(train_sample.index).sample(n=min(20, len(meta_df)//4))\n",
    "\n",
    "# Create dataset dictionary\n",
    "sample_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_sample),\n",
    "    \"test\": Dataset.from_pandas(test_sample)\n",
    "})\n",
    "\n",
    "# Load Whisper components\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large\", language=\"english\", task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\", language=\"english\", task=\"transcribe\")\n",
    "\n",
    "# Load and process audio files\n",
    "def load_audio(example):\n",
    "    audio_path = example[\"audio_filepath\"]\n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=16000)\n",
    "        return {\"audio\": {\"array\": audio, \"sampling_rate\": sr}}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "        return {\"audio\": {\"array\": np.zeros(1600), \"sampling_rate\": 16000}}\n",
    "\n",
    "# Prepare features and labels\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# Process the dataset\n",
    "sample_dataset = sample_dataset.map(\n",
    "    load_audio,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "sample_dataset = sample_dataset.cast_column(\n",
    "    \"audio\",\n",
    "    Audio(sampling_rate=16000)\n",
    ")\n",
    "\n",
    "processed_dataset = sample_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=sample_dataset.column_names[\"train\"],\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "# Data Collator\n",
    "def data_collator(features):\n",
    "    input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "    batch = processor.feature_extractor.pad(input_features, padding=\"longest\", return_tensors=\"pt\")\n",
    "    labels = [feature[\"labels\"] for feature in features]\n",
    "    batch[\"labels\"] = tokenizer.pad({\"input_ids\": labels}, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return batch\n",
    "\n",
    "# Training arguments\n",
    "seq2seq_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_PATH,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=3,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,  # GPU should support this on HPC\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=25,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "# Model Initialization\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
    "\n",
    "# Trainer Initialization\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=seq2seq_training_args,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(MODEL_OUTPUT_PATH)\n",
    "print(f\"Model saved to {MODEL_OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
