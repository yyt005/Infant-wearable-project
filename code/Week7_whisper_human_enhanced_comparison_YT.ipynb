{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bc4508-c69d-4bfa-9557-76082b16c03b",
   "metadata": {},
   "source": [
    "# Parent-Infant Interaction Analysis Pipeline\n",
    "\n",
    "## Setup and Dependencies\n",
    "```python\n",
    "import torch\n",
    "import whisper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "from scipy.signal import butter, filtfilt\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import soundfile as sf\n",
    "from typing import Union, List, Tuple\n",
    "```\n",
    "Key dependencies:\n",
    "- OpenAI Whisper for speech recognition\n",
    "- PyDub for audio processing\n",
    "- FFmpeg for audio file manipulation\n",
    "- Pandas for data handling\n",
    "- NumPy for numerical operations\n",
    "\n",
    "## Pipeline Overview\n",
    "This notebook implements a three-stage pipeline for analyzing parent-infant interactions:\n",
    "1. Audio Preprocessing\n",
    "2. Speech Recognition\n",
    "3. Accuracy Analysis\n",
    "\n",
    "### 1. Audio Preprocessing (`AudioPreprocessor` class)\n",
    "Prepares audio files for optimal speech recognition:\n",
    "- Converts stereo to mono (required for Whisper)\n",
    "- Adjusts sample rate to 16kHz\n",
    "- Normalizes volume\n",
    "- Optional silence removal (not recommended for interaction analysis)\n",
    "\n",
    "Example usage:\n",
    "```python\n",
    "preprocessor = AudioPreprocessor(input_file)\n",
    "processed_file = preprocessor.process_audio(\n",
    "    output_dir=output_dir,\n",
    "    remove_silence=False  # Keep natural pauses\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. Speech Recognition (`transcribe_audio` function)\n",
    "Uses Whisper ASR with parent-infant specific settings:\n",
    "- Large-v2 model for accuracy\n",
    "- Custom prompt for infant vocalizations\n",
    "- Word-level timestamps\n",
    "- Speaker detection capabilities\n",
    "\n",
    "Output includes:\n",
    "- Transcribed text\n",
    "- Timing information\n",
    "- Optional speaker labels\n",
    "\n",
    "### 3. Accuracy Analysis (`EnhancedTranscriptionComparator` class)\n",
    "Compares Whisper output with human transcriptions:\n",
    "- Text similarity scoring\n",
    "- Word-level accuracy metrics\n",
    "- Detailed analysis reports\n",
    "\n",
    "### Key Metrics\n",
    "1. Text Similarity\n",
    "   - SequenceMatcher score (Ratcliff/Obershelp algorithm)\n",
    "   - Formula: ratio = 2.0 * M / T\n",
    "     * M = sum of lengths of matched subsequences\n",
    "     * T = total length of both strings combined\n",
    "\n",
    "2. Word-level Analysis\n",
    "   - Word count comparison\n",
    "   - Correct words identification\n",
    "   - Mismatch detection\n",
    "   - Accuracy percentage calculation\n",
    "\n",
    "### Output Format\n",
    "CSV file with columns:\n",
    "- Time Range\n",
    "- Whisper Transcription\n",
    "- Human Transcription\n",
    "- SequenceMatcher Score\n",
    "- Word Count (Human)\n",
    "- Word Count (Whisper)\n",
    "- Correct Words\n",
    "- Mismatches\n",
    "- Accuracy (%)\n",
    "- Comments/Notes\n",
    "\n",
    "## Results Interpretation\n",
    "The comparison output provides:\n",
    "- Time-aligned transcriptions\n",
    "- Word-level accuracy metrics\n",
    "- Quality indicators\n",
    "- Automated issue detection\n",
    "\n",
    "Example metrics from current run:\n",
    "- Total segments: 95\n",
    "- Average accuracy: 65.3%\n",
    "- High quality matches: 39\n",
    "- Low quality matches: 38\n",
    "\n",
    "## Future Improvements\n",
    "Consider:\n",
    "- GPU acceleration for faster processing\n",
    "- Enhanced speaker detection\n",
    "- Controlled vocabulary for infant sounds\n",
    "- Batch processing capabilities\n",
    "\n",
    "## Suggested File Structure\n",
    "\n",
    "```\n",
    "File Structure Overview:\n",
    "project/wearable/\n",
    "├── media/[MONTH]/                    # Audio files\n",
    "│   ├── IW_[ID]_[MONTH]_TL.wav       # Original audio\n",
    "│   └── processed/                    # Processed audio files\n",
    "│       ├── IW_[ID]_[MONTH]_TL_mono.wav\n",
    "│       ├── IW_[ID]_[MONTH]_TL_mono_16k.wav\n",
    "│       └── IW_[ID]_[MONTH]_TL_mono_16k_normalized.wav\n",
    "│\n",
    "└── transcription/[MONTH]/            # Transcription files\n",
    "    ├── IW_[ID]_[MONTH]_whisper_results_without_speaker.csv  # Whisper output\n",
    "    └── IW_[ID]_[MONTH]_transcription_comparison.csv         # Analysis results\n",
    "\n",
    "Naming Convention:\n",
    "- [ID]: Participant ID (e.g., 002)\n",
    "- [MONTH]: Recording month (e.g., 12mon)\n",
    "```\n",
    "\n",
    "## Usage Workflow\n",
    "\n",
    "### 1. Preprocess Audio\n",
    "```python\n",
    "# Initialize and process audio\n",
    "preprocessor = AudioPreprocessor(\"/path/to/media/12mon/IW_002_12_TL.wav\")\n",
    "processed_file = preprocessor.process_audio(\n",
    "    output_dir=\"/path/to/media/12mon/processed\",\n",
    "    remove_silence=False    # Keep silence for interaction analysis\n",
    ")\n",
    "# Output: IW_002_12_TL_mono_16k_normalized.wav\n",
    "```\n",
    "\n",
    "### 2. Transcribe Audio\n",
    "```python\n",
    "# Run Whisper transcription\n",
    "df, result = transcribe_audio(\n",
    "    audio_path=\"/path/to/processed/IW_002_12_TL_mono_16k_normalized.wav\",\n",
    "    speaker_hints=[\"[Mother]\", \"[Father]\", \"[Baby]\"]\n",
    ")\n",
    "df.to_csv(\"002_12_whisper_results.csv\", index=False)\n",
    "```\n",
    "\n",
    "### 3. Compare Transcriptions\n",
    "```python\n",
    "# Compare with human transcription\n",
    "results_df, stats = compare_and_export(\n",
    "    whisper_df,    # Whisper output\n",
    "    human_df,      # Human transcription\n",
    "    \"002_12_transcription_comparison.csv\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Expected Outputs\n",
    "1. Processed Audio: `*_mono_16k_normalized.wav`\n",
    "2. Whisper Output: `*_whisper_results.csv`\n",
    "3. Analysis: `*_transcription_comparison.csv`\n",
    "\n",
    "\n",
    "## Technical Notes\n",
    "- Time tolerance: 0.5 seconds for segment matching\n",
    "- Similarity threshold: 0.3 for considering matches\n",
    "- Text cleaning includes:\n",
    "  * Case normalization\n",
    "  * Punctuation removal\n",
    "  * Common transcription variant standardization\n",
    "- High quality matches: Accuracy > 80%\n",
    "- Low quality matches: Accuracy < 60%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5731dc3b-1901-4772-9f6b-b08bfaf3f1a2",
   "metadata": {},
   "source": [
    "###preprocessing\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, input_path: Union[str, Path]):\n",
    "        \"\"\"Initialize audio preprocessor with input file path\"\"\"\n",
    "        self.input_path = Path(input_path)\n",
    "        self.setup_logging()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging configuration\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def process_audio(self, \n",
    "                     output_dir: Union[str, Path],\n",
    "                     apply_noise_reduction: bool = True,\n",
    "                     noise_reduction_strength: float = 0.21,\n",
    "                     target_volume: float = 1.5) -> Path:\n",
    "        \"\"\"\n",
    "        Process audio using FFmpeg filter chain:\n",
    "        1. Convert to mono\n",
    "        2. Adjust sample rate to 16kHz\n",
    "        3. Optional noise reduction\n",
    "        4. Normalize volume\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save processed audio\n",
    "            apply_noise_reduction: Whether to apply noise reduction\n",
    "            noise_reduction_strength: Strength of noise reduction (0.01 to 1.0)\n",
    "            target_volume: Target volume for normalization\n",
    "            \n",
    "        Returns:\n",
    "            Path to processed audio file\n",
    "        \"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Construct output path\n",
    "        output_path = output_dir / f\"{self.input_path.stem}_processed.wav\"\n",
    "        \n",
    "        # Build FFmpeg filter chain\n",
    "        filters = [\n",
    "            'aresample=16000',  # Convert to 16kHz\n",
    "            'aformat=sample_fmts=s16:channel_layouts=mono',  # Convert to mono\n",
    "        ]\n",
    "        \n",
    "        # Add noise reduction if requested\n",
    "        if apply_noise_reduction:\n",
    "            filters.append(f'anlmdn=s={noise_reduction_strength}')\n",
    "        \n",
    "        # Add volume normalization\n",
    "        filters.append(f'volume={target_volume}')\n",
    "        \n",
    "        # Join filters into a single filter chain\n",
    "        filter_chain = ','.join(filters)\n",
    "        \n",
    "        # Build FFmpeg command\n",
    "        command = [\n",
    "            'ffmpeg',\n",
    "            '-i', str(self.input_path),\n",
    "            '-af', filter_chain,\n",
    "            '-ar', '16000',  # Ensure 16kHz output\n",
    "            '-ac', '1',      # Ensure mono output\n",
    "            '-y',            # Overwrite output if exists\n",
    "            str(output_path)\n",
    "        ]\n",
    "        \n",
    "        # Log the command for debugging\n",
    "        self.logger.info(\"Running FFmpeg command:\")\n",
    "        self.logger.info(' '.join(command))\n",
    "        \n",
    "        try:\n",
    "            # Run FFmpeg command\n",
    "            subprocess.run(command, \n",
    "                         check=True,\n",
    "                         capture_output=True,\n",
    "                         text=True)\n",
    "            \n",
    "            self.logger.info(f\"Successfully processed audio to: {output_path}\")\n",
    "            return output_path\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.logger.error(f\"FFmpeg error: {e.stderr}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during processing: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = Path(\"/Users/yueyan/Documents/project/wearable/media/12mon/raw/IW_002_12_TL.wav\")\n",
    "    output_dir = Path(\"/Users/yueyan/Documents/project/wearable/media/12mon/processed\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize preprocessor\n",
    "        preprocessor = AudioPreprocessor(input_file)\n",
    "        \n",
    "        # Process audio\n",
    "        processed_file = preprocessor.process_audio(\n",
    "            output_dir=output_dir,\n",
    "            apply_noise_reduction=True,\n",
    "            noise_reduction_strength=0.01,  # Adjust between 0.01 and 1.0\n",
    "            target_volume=1.5              # Adjust as needed\n",
    "        )\n",
    "        \n",
    "        print(f\"Processed file: {processed_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Processing failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c486a-cdf1-4683-86ce-3c76b226755d",
   "metadata": {},
   "source": [
    "###whisper transciption with speaker recognition\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "\n",
    "def transcribe_audio(audio_path, speaker_hints=None):\n",
    "    \"\"\"Transcribe audio using Whisper with enhanced speaker detection\"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    model = whisper.load_model(\"large-v2\", device=\"cpu\")\n",
    "    \n",
    "    # Keep the parent-infant context in the prompt but make it more transcription-focused\n",
    "    initial_prompt = \"\"\"This is a parent-infant interaction recording. \n",
    "    Please transcribe all speech, including infant vocalizations, parent speech, and any notable sounds.\n",
    "    Include both verbal and non-verbal vocalizations.\"\"\"\n",
    "    \n",
    "    print(\"Transcribing...\")\n",
    "    result = model.transcribe(\n",
    "        audio_path,\n",
    "        word_timestamps=True,\n",
    "        verbose=True,\n",
    "        initial_prompt=initial_prompt,\n",
    "        language=\"en\",\n",
    "        task=\"transcribe\"\n",
    "    )\n",
    "    \n",
    "    # Process segments\n",
    "    segments = []\n",
    "    \n",
    "    # First pass: Get clean transcriptions with timing\n",
    "    for segment in result[\"segments\"]:\n",
    "        text = segment[\"text\"].strip()\n",
    "        start = segment[\"start\"]\n",
    "        end = segment[\"end\"]\n",
    "        duration = end - start\n",
    "        \n",
    "        # Skip empty segments\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        segment_info = {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'start_time': str(timedelta(seconds=int(start))),\n",
    "            'end_time': str(timedelta(seconds=int(end))),\n",
    "            'text': text,\n",
    "            'duration': duration\n",
    "        }\n",
    "        \n",
    "        # Optional: Add speaker detection without modifying the original text\n",
    "        if speaker_hints:\n",
    "            speaker = detect_speaker(text, speaker_hints)\n",
    "            segment_info['speaker'] = speaker\n",
    "        \n",
    "        segments.append(segment_info)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(segments)\n",
    "    \n",
    "    return df, result\n",
    "\n",
    "def detect_speaker(text, speaker_hints):\n",
    "    \"\"\"Separate function for speaker detection that doesn't modify the transcription\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Basic speaker detection logic\n",
    "    if any(word in text_lower for word in ['crying', 'cries', 'waa']):\n",
    "        return '[Baby Crying]'\n",
    "    elif any(word in text_lower for word in ['laugh', 'giggle']):\n",
    "        return '[Baby Laughing]'\n",
    "    elif any(word in text_lower for word in ['goo', 'gah', 'bah', 'coo']):\n",
    "        return '[Infant Vocalization]'\n",
    "    elif '[mother]' in text_lower or 'mom' in text_lower:\n",
    "        return '[Mother]'\n",
    "    elif '[father]' in text_lower or 'dad' in text_lower:\n",
    "        return '[Father]'\n",
    "    else:\n",
    "        return '[Unspecified]'\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    speaker_hints = [\n",
    "        \"[Mother]\", \"[Father]\", \"[Baby]\",\n",
    "        \"[Infant Vocalization]\", \"[Baby Crying]\", \"[Baby Laughing]\"\n",
    "    ]\n",
    "\n",
    "    ###replace with your audio file\n",
    "    try:\n",
    "        df, result = transcribe_audio(\n",
    "            \"/Users/yueyan/Documents/project/wearable/media/12mon/your file name\",\n",
    "            speaker_hints=speaker_hints\n",
    "        )\n",
    "        \n",
    "        # Save results; replace with your path\n",
    "        df.to_csv(\"/Users/yueyan/Documents/project/wearable/transcription/12mon/your file name to be saved\", index=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nTranscription Statistics:\")\n",
    "        print(f\"Total segments: {len(df)}\")\n",
    "        \n",
    "        # If speaker detection is enabled\n",
    "        if 'speaker' in df.columns:\n",
    "            print(\"\\nSpeaker distribution:\")\n",
    "            print(df['speaker'].value_counts())\n",
    "        \n",
    "        print(\"\\nFirst few transcriptions:\")\n",
    "        print(df[['start_time', 'end_time', 'text']].head())\n",
    "        \n",
    "        # Optionally display with speakers if available\n",
    "        if 'speaker' in df.columns:\n",
    "            print(\"\\nFirst few transcriptions with speakers:\")\n",
    "            print(df[['start_time', 'end_time', 'speaker', 'text']].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d8961-e6c8-469d-ba0b-697ff11785eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##whisper transcription without speaker recognition\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"Transcribe audio using Whisper\"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    model = whisper.load_model(\"large-v2\", device=\"cpu\")\n",
    "    \n",
    "    # Keep the parent-infant context in the prompt but make it more transcription-focused\n",
    "    initial_prompt = \"\"\"This is a parent-infant interaction recording. \"\"\"\n",
    "#    Please transcribe all speech, including infant vocalizations, parent speech, and any notable sounds.\n",
    "#    Include both verbal and non-verbal vocalizations.\"\"\"\n",
    "    \n",
    "    print(\"Transcribing...\")\n",
    "    result = model.transcribe(\n",
    "        audio_path,\n",
    "        word_timestamps=True,\n",
    "        verbose=True,\n",
    "        initial_prompt=initial_prompt,\n",
    "        language=\"en\",\n",
    "        task=\"transcribe\"\n",
    "    )\n",
    "    \n",
    "    # Process segments\n",
    "    segments = []\n",
    "    \n",
    "    # First pass: Get clean transcriptions with timing\n",
    "    for segment in result[\"segments\"]:\n",
    "        text = segment[\"text\"].strip()\n",
    "        start = segment[\"start\"]\n",
    "        end = segment[\"end\"]\n",
    "        duration = end - start\n",
    "        \n",
    "        # Skip empty segments\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        segment_info = {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'start_time': str(timedelta(seconds=int(start))),\n",
    "            'end_time': str(timedelta(seconds=int(end))),\n",
    "            'text': text,\n",
    "            'duration': duration\n",
    "        }\n",
    "        \n",
    "        segments.append(segment_info)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(segments)\n",
    "    \n",
    "    return df, result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df, result = transcribe_audio(\n",
    "            \"/Users/yueyan/Documents/project/wearable/media/12mon/raw/IW_009_12_TL.wav\"\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        df.to_csv(\"/Users/yueyan/Documents/project/wearable/transcription/12mon/IW_009_12_whisper_wopre_111424_v2_wos.csv\", index=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nTranscription Statistics:\")\n",
    "        print(f\"Total segments: {len(df)}\")\n",
    "        \n",
    "        print(\"\\nFirst few transcriptions:\")\n",
    "        print(df[['start_time', 'end_time', 'text']].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f1685-fc99-455a-88ed-239537ee6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "###load datasets\n",
    "whisper_df = pd.read_csv(\"/specify your path/and your file name\")\n",
    "human_df = pd.read_csv(\"/specify your path/and your file name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0be6b912-4d90-4314-b6c1-cfe298211fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing segments: 118it [00:00, 2660.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison Summary:\n",
      "Total segments analyzed: 95\n",
      "Average accuracy: 65.3%\n",
      "Average similarity score: 0.756\n",
      "High quality matches (>80%): 39\n",
      "Low quality matches (<60%): 38\n",
      "\n",
      "Sample comparison results:\n",
      "    Time Range Human Transcription Whisper Transcription  Accuracy (%)\n",
      "0  157.7-158.4             squeak.               Squeak!         100.0\n",
      "1  164.1-165.3               yeah.                 Yeah.         100.0\n",
      "2  172.5-174.0  yeah it's a rings.    Yeah, it's a ring.          75.0\n",
      "3  177.9-178.8          thank you.            Thank you.         100.0\n",
      "4  182.3-183.0          thank you.            Thank you.         100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "class EnhancedTranscriptionComparator:\n",
    "    def __init__(self, whisper_df: pd.DataFrame, human_df: pd.DataFrame):\n",
    "        \"\"\"Initialize with enhanced text cleaning and comparison capabilities\"\"\"\n",
    "        self.whisper_data = whisper_df.copy()\n",
    "        self.human_data = human_df.copy()\n",
    "        \n",
    "        # Pre-clean the data\n",
    "        self.human_data = self.human_data[self.human_data['text'].notna()]\n",
    "        if 'end ' in self.human_data.columns:\n",
    "            self.human_data = self.human_data.rename(columns={'end ': 'end'})\n",
    "        \n",
    "        # Convert times to numeric\n",
    "        self.human_data['start'] = pd.to_numeric(self.human_data['start'])\n",
    "        self.human_data['end'] = pd.to_numeric(self.human_data['end'])\n",
    "        \n",
    "        # Clean text and store originals\n",
    "        self.whisper_data['clean_text'] = self.whisper_data['text'].apply(self.clean_text)\n",
    "        self.human_data['clean_text'] = self.human_data['text'].apply(self.clean_text)\n",
    "        self.whisper_data['original_text'] = self.whisper_data['text']\n",
    "        self.human_data['original_text'] = self.human_data['text']\n",
    "        \n",
    "        # Find overlap range\n",
    "        self.overlap_start = max(self.whisper_data['start'].min(), self.human_data['start'].min())\n",
    "        self.overlap_end = min(self.whisper_data['end'].max(), self.human_data['end'].max())\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"Enhanced text cleaning function\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "            \n",
    "        text = str(text).lower()\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        replacements = {\n",
    "            'uhm': 'um', 'uhh': 'uh', 'hmm': 'hm',\n",
    "            'mhm': 'mm', 'yeah': 'yes', 'yah': 'yes',\n",
    "            'nah': 'no'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = re.sub(r'\\b' + old + r'\\b', new, text)\n",
    "            \n",
    "        return text\n",
    "\n",
    "    def get_word_metrics(self, human_text: str, whisper_text: str) -> Dict[str, int]:\n",
    "        \"\"\"Calculate detailed word-level metrics\"\"\"\n",
    "        human_words = set(self.clean_text(human_text).split())\n",
    "        whisper_words = set(self.clean_text(whisper_text).split())\n",
    "        \n",
    "        correct_words = len(human_words.intersection(whisper_words))\n",
    "        total_words_human = len(human_words)\n",
    "        total_words_whisper = len(whisper_words)\n",
    "        mismatches = max(total_words_human, total_words_whisper) - correct_words\n",
    "        \n",
    "        return {\n",
    "            'word_count_human': total_words_human,\n",
    "            'word_count_whisper': total_words_whisper,\n",
    "            'correct_words': correct_words,\n",
    "            'mismatches': mismatches\n",
    "        }\n",
    "\n",
    "    def generate_comparison_results(self, time_tolerance: float = 0.5, \n",
    "                                  similarity_threshold: float = 0.3) -> pd.DataFrame:\n",
    "        \"\"\"Generate comprehensive comparison results for spreadsheet\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        human_segments = self.human_data[\n",
    "            (self.human_data['start'] >= self.overlap_start) &\n",
    "            (self.human_data['end'] <= self.overlap_end)\n",
    "        ]\n",
    "        \n",
    "        whisper_segments = self.whisper_data[\n",
    "            (self.whisper_data['start'] >= self.overlap_start) &\n",
    "            (self.whisper_data['end'] <= self.overlap_end)\n",
    "        ]\n",
    "        \n",
    "        for _, human_seg in tqdm(human_segments.iterrows(), desc=\"Analyzing segments\"):\n",
    "            potential_matches = whisper_segments[\n",
    "                (whisper_segments['start'] >= human_seg['start'] - time_tolerance) &\n",
    "                (whisper_segments['start'] <= human_seg['end'] + time_tolerance)\n",
    "            ]\n",
    "            \n",
    "            for _, whisper_seg in potential_matches.iterrows():\n",
    "                similarity = SequenceMatcher(\n",
    "                    None,\n",
    "                    human_seg['clean_text'],\n",
    "                    whisper_seg['clean_text']\n",
    "                ).ratio()\n",
    "                \n",
    "                if similarity > similarity_threshold:\n",
    "                    # Get word-level metrics\n",
    "                    word_metrics = self.get_word_metrics(\n",
    "                        human_seg['original_text'],\n",
    "                        whisper_seg['original_text']\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    accuracy = (word_metrics['correct_words'] / word_metrics['word_count_human'] * 100) \\\n",
    "                        if word_metrics['word_count_human'] > 0 else 0\n",
    "                    \n",
    "                    # Generate comments\n",
    "                    comments = []\n",
    "                    if similarity < 0.5:\n",
    "                        comments.append(\"Low similarity score\")\n",
    "                    if abs(word_metrics['word_count_human'] - word_metrics['word_count_whisper']) > 3:\n",
    "                        comments.append(\"Significant word count difference\")\n",
    "                    if accuracy < 60:\n",
    "                        comments.append(\"Low accuracy\")\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Time Range': f\"{human_seg['start']:.1f}-{human_seg['end']:.1f}\",\n",
    "                        'Whisper Transcription': whisper_seg['original_text'],\n",
    "                        'Human Transcription': human_seg['original_text'],\n",
    "                        'SequenceMatcher Score': round(similarity, 3),\n",
    "                        'Word Count (Human)': word_metrics['word_count_human'],\n",
    "                        'Word Count (Whisper)': word_metrics['word_count_whisper'],\n",
    "                        'Correct Words': word_metrics['correct_words'],\n",
    "                        'Mismatches': word_metrics['mismatches'],\n",
    "                        'Accuracy (%)': round(accuracy, 1),\n",
    "                        'Comments/Notes': \"; \".join(comments) if comments else \"OK\"\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "def compare_and_export(whisper_file: pd.DataFrame, human_file: pd.DataFrame, \n",
    "                      output_path: str = \"transcription_comparison.csv\") -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"Compare transcriptions and export results\"\"\"\n",
    "    # Initialize comparator\n",
    "    comparator = EnhancedTranscriptionComparator(whisper_file, human_file)\n",
    "    \n",
    "    # Generate comparison results\n",
    "    results_df = comparator.generate_comparison_results()\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    stats = {\n",
    "        'Total Segments': len(results_df),\n",
    "        'Average Accuracy': results_df['Accuracy (%)'].mean(),\n",
    "        'Average Similarity': results_df['SequenceMatcher Score'].mean(),\n",
    "        'High Quality Matches': len(results_df[results_df['Accuracy (%)'] > 80]),\n",
    "        'Low Quality Matches': len(results_df[results_df['Accuracy (%)'] < 60])\n",
    "    }\n",
    "    \n",
    "    # Export to CSV\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nComparison Summary:\")\n",
    "    print(f\"Total segments analyzed: {stats['Total Segments']}\")\n",
    "    print(f\"Average accuracy: {stats['Average Accuracy']:.1f}%\")\n",
    "    print(f\"Average similarity score: {stats['Average Similarity']:.3f}\")\n",
    "    print(f\"High quality matches (>80%): {stats['High Quality Matches']}\")\n",
    "    print(f\"Low quality matches (<60%): {stats['Low Quality Matches']}\")\n",
    "    \n",
    "    return results_df, stats\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume whisper_df and human_df are your input DataFrames; replace with your desired path for the comparison results to be saved\n",
    "    results_df, stats = compare_and_export(whisper_df, human_df, \"/Users/yueyan/Documents/project/wearable/transcription/12mon/002_12_transcription_comparison.csv\")\n",
    "    \n",
    "    # Display first few results\n",
    "    print(\"\\nSample comparison results:\")\n",
    "    print(results_df[['Time Range', 'Human Transcription', \n",
    "                     'Whisper Transcription', 'Accuracy (%)']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109b288-71ba-4f5e-b7e0-a2df248ba215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
