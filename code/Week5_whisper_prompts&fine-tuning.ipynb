{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8192eba-1b2c-4bb7-a202-1b8611e71d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a9f76-5476-4c85-b556-10f53bae9094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e626984e-9756-450b-af23-9b33a754eb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yueyan/.pyenv/versions/3.9.7/lib/python3.9/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 129\u001b[0m\n\u001b[1;32m    123\u001b[0m speaker_hints \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Mother]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Father]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Baby]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Infant Vocalization]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Baby Crying]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Baby Laughing]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m ]\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     df, result \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/yueyan/Documents/project/wearable/media/025_04/IW_025_04_YT.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspeaker_hints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker_hints\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/yueyan/Documents/project/wearable/transcription/025_04_whisper_results_with_speaker.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mtranscribe_audio\u001b[0;34m(audio_path, speaker_hints)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transcribe audio using Whisper with enhanced speaker detection\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlarge-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Add initial prompt to encourage speaker labeling\u001b[39;00m\n\u001b[1;32m     13\u001b[0m initial_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThis is a parent-infant interaction transcript. \u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124mSpeakers include Mother, Father, and Baby. \u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mFormat: [Speaker] followed by speech content.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/whisper/__init__.py:154\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint_file\n\u001b[1;32m    153\u001b[0m dims \u001b[38;5;241m=\u001b[39m ModelDimensions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheckpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 154\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWhisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alignment_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/whisper/model.py:263\u001b[0m, in \u001b[0;36mWhisper.__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m=\u001b[39m dims\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m AudioEncoder(\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_mels,\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_layer,\n\u001b[1;32m    262\u001b[0m )\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mTextDecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# use the last half among the decoder layers for time alignment by default;\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# to use a specific set of heads, see `set_alignment_heads()` below.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m all_heads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_head, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool\n\u001b[1;32m    274\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/whisper/model.py:217\u001b[0m, in \u001b[0;36mTextDecoder.__init__\u001b[0;34m(self, n_vocab, n_ctx, n_state, n_head, n_layer)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(n_vocab, n_state)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_state))\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks: Iterable[ResidualAttentionBlock] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 217\u001b[0m     [\n\u001b[1;32m    218\u001b[0m         ResidualAttentionBlock(n_state, n_head, cross_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layer)\n\u001b[1;32m    220\u001b[0m     ]\n\u001b[1;32m    221\u001b[0m )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state)\n\u001b[1;32m    224\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_ctx)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf)\u001b[38;5;241m.\u001b[39mtriu_(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/whisper/model.py:218\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(n_vocab, n_state)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_state))\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks: Iterable[ResidualAttentionBlock] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    217\u001b[0m     [\n\u001b[0;32m--> 218\u001b[0m         \u001b[43mResidualAttentionBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layer)\n\u001b[1;32m    220\u001b[0m     ]\n\u001b[1;32m    221\u001b[0m )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state)\n\u001b[1;32m    224\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_ctx)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf)\u001b[38;5;241m.\u001b[39mtriu_(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/whisper/model.py:156\u001b[0m, in \u001b[0;36mResidualAttentionBlock.__init__\u001b[0;34m(self, n_state, n_head, cross_attention)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_ln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state) \u001b[38;5;28;01mif\u001b[39;00m cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m n_mlp \u001b[38;5;241m=\u001b[39m n_state \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m--> 156\u001b[0m     \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mlp\u001b[49m\u001b[43m)\u001b[49m, nn\u001b[38;5;241m.\u001b[39mGELU(), Linear(n_mlp, n_state)\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_ln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/linear.py:112\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/modules/linear.py:118\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/torch/nn/init.py:518\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    516\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "\n",
    "###preprocessing + prompts \n",
    "def transcribe_audio(audio_path, speaker_hints=None):\n",
    "    \"\"\"Transcribe audio using Whisper with enhanced speaker detection\"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    model = whisper.load_model(\"large-v2\", device=\"cpu\")\n",
    "    \n",
    "    # Add initial prompt to encourage speaker labeling\n",
    "    initial_prompt = \"\"\"This is a parent-infant interaction transcript. \n",
    "    Speakers include Mother, Father, and Baby. \n",
    "    Format: [Speaker] followed by speech content.\"\"\"\n",
    "    \n",
    "    print(\"Transcribing...\")\n",
    "    result = model.transcribe(\n",
    "        audio_path,\n",
    "        word_timestamps=True,\n",
    "        verbose=True,\n",
    "        initial_prompt=initial_prompt,\n",
    "        language=\"en\",\n",
    "        task=\"transcribe\"\n",
    "    )\n",
    "    \n",
    "    # Process segments\n",
    "    segments = []\n",
    "    if speaker_hints is None:\n",
    "        speaker_hints = [\n",
    "            \"[Mother]\", \"[Father]\", \"[Baby]\",\n",
    "            \"[Infant Vocalization]\", \"[Baby Crying]\", \"[Baby Laughing]\"\n",
    "        ]\n",
    "    \n",
    "    # Helper function for speaker detection\n",
    "    def detect_speaker(text, previous_speaker=None):\n",
    "        # Check for explicit speaker markers\n",
    "        for hint in speaker_hints:\n",
    "            if hint.lower() in text.lower():\n",
    "                return hint\n",
    "            \n",
    "        # Acoustic and content-based heuristics\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # Check for infant-specific patterns\n",
    "        baby_patterns = ['goo', 'gah', 'bah', 'mama', 'dada', 'babbling', 'crying', 'laughing']\n",
    "        if any(pattern in text.lower() for pattern in baby_patterns):\n",
    "            return '[Baby]'\n",
    "        \n",
    "        # Check for parent-specific patterns\n",
    "        parent_patterns = [\n",
    "            'good job', 'look at', 'here we go', 'that\\'s right',\n",
    "            'can you', 'let\\'s', 'sweetie', 'honey', 'baby'\n",
    "        ]\n",
    "        if any(pattern in text.lower() for pattern in parent_patterns):\n",
    "            return previous_speaker if previous_speaker in ['[Mother]', '[Father]'] else '[Mother]'\n",
    "        \n",
    "        # Context continuation\n",
    "        if previous_speaker and len(text.split()) < 5:  # Short utterances likely continue previous speaker\n",
    "            return previous_speaker\n",
    "            \n",
    "        return previous_speaker if previous_speaker else \"[Unknown]\"\n",
    "    \n",
    "    # Process segments with context\n",
    "    previous_speaker = None\n",
    "    min_segment_duration = 0.3  # Minimum duration for a valid segment\n",
    "    \n",
    "    for i, segment in enumerate(result[\"segments\"]):\n",
    "        text = segment[\"text\"].strip()\n",
    "        start = segment[\"start\"]\n",
    "        end = segment[\"end\"]\n",
    "        duration = end - start\n",
    "        \n",
    "        # Skip very short segments that might be noise\n",
    "        if duration < min_segment_duration:\n",
    "            continue\n",
    "        \n",
    "        # Enhanced speaker detection\n",
    "        detected_speaker = detect_speaker(text, previous_speaker)\n",
    "        \n",
    "        # Update previous speaker if we have a confident detection\n",
    "        if detected_speaker != \"[Unknown]\":\n",
    "            previous_speaker = detected_speaker\n",
    "        \n",
    "        # Add segment info\n",
    "        segment_info = {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'start_time': str(timedelta(seconds=int(start))),\n",
    "            'end_time': str(timedelta(seconds=int(end))),\n",
    "            'speaker': detected_speaker,\n",
    "            'text': text,\n",
    "            'duration': duration,\n",
    "            'words_per_second': len(text.split()) / duration if duration > 0 else 0\n",
    "        }\n",
    "        \n",
    "        # Add confidence metrics\n",
    "        segment_info['confidence'] = 'high' if detected_speaker != \"[Unknown]\" else 'low'\n",
    "        \n",
    "        segments.append(segment_info)\n",
    "    \n",
    "    # Post-process segments to improve speaker consistency\n",
    "    df = pd.DataFrame(segments)\n",
    "    \n",
    "    # Smooth speaker labels using a rolling window\n",
    "    window_size = 3\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i]['speaker'] == \"[Unknown]\":\n",
    "            # Look at surrounding segments\n",
    "            start_idx = max(0, i - window_size)\n",
    "            end_idx = min(len(df), i + window_size + 1)\n",
    "            window = df.iloc[start_idx:end_idx]\n",
    "            \n",
    "            # Count speaker occurrences in window\n",
    "            speaker_counts = window['speaker'].value_counts()\n",
    "            if len(speaker_counts) > 0 and speaker_counts.index[0] != \"[Unknown]\":\n",
    "                df.at[i, 'speaker'] = speaker_counts.index[0]\n",
    "    \n",
    "    return df, result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    speaker_hints = [\n",
    "        \"[Mother]\", \"[Father]\", \"[Baby]\",\n",
    "        \"[Infant Vocalization]\", \"[Baby Crying]\", \"[Baby Laughing]\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        df, result = transcribe_audio(\n",
    "            \"/Users/yueyan/Documents/project/wearable/media/025_04/IW_025_04_YT.wav\",\n",
    "            speaker_hints=speaker_hints\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        df.to_csv(\"/Users/yueyan/Documents/project/wearable/transcription/025_04_whisper_results_with_speaker.csv\", index=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nTranscription Statistics:\")\n",
    "        print(f\"Total segments: {len(df)}\")\n",
    "        print(\"\\nSpeaker distribution:\")\n",
    "        print(df['speaker'].value_counts())\n",
    "        \n",
    "        print(\"\\nFirst few transcriptions:\")\n",
    "        print(df[['start_time', 'end_time', 'speaker', 'text']].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2af5f06-30ea-410d-9b67-e88868ae85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "####reliability checks \n",
    "class TranscriptionComparator:\n",
    "    def __init__(self, whisper_df: pd.DataFrame, human_df: pd.DataFrame):\n",
    "        \"\"\"Initialize with enhanced text cleaning\"\"\"\n",
    "        self.whisper_data = whisper_df.copy()\n",
    "        self.human_data = human_df.copy()\n",
    "        \n",
    "        # Pre-clean the data\n",
    "        #Removes rows with missing text\n",
    "        #fix space after \"end\" (could be deleted)\n",
    "        self.human_data = self.human_data[self.human_data['text'].notna()]\n",
    "        self.human_data = self.human_data.rename(columns={'end ': 'end'})\n",
    "        \n",
    "        # Convert times to numeric\n",
    "        # Ensures time values are numeric for calculations\n",
    "        self.human_data['start'] = pd.to_numeric(self.human_data['start'])\n",
    "        self.human_data['end'] = pd.to_numeric(self.human_data['end'])\n",
    "        \n",
    "        # Clean text thoroughly\n",
    "        # while preserving originals\n",
    "        self.whisper_data['clean_text'] = self.whisper_data['text'].apply(self.clean_text)\n",
    "        self.human_data['clean_text'] = self.human_data['text'].apply(self.clean_text)\n",
    "        \n",
    "        # Store original text for reference\n",
    "        self.whisper_data['original_text'] = self.whisper_data['text']\n",
    "        self.human_data['original_text'] = self.human_data['text']\n",
    "        \n",
    "        # Find overlap range\n",
    "        # Determines the time range where both transcriptions overlap\n",
    "        # Uses max of starts and min of ends to find common time period\n",
    "        self.overlap_start = max(\n",
    "            self.whisper_data['start'].min(),\n",
    "            self.human_data['start'].min()\n",
    "        )\n",
    "        self.overlap_end = min(\n",
    "            self.whisper_data['end'].max(),\n",
    "            self.human_data['end'].max()\n",
    "        )\n",
    "        \n",
    "        print(f\"Data loaded and cleaned. Time range: {self.overlap_start:.2f}s - {self.overlap_end:.2f}s\")\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        text cleaning function\n",
    "        - Converts to lowercase\n",
    "        - Removes punctuation\n",
    "        - Removes extra whitespace\n",
    "        - Standardizes common transcription artifacts\n",
    "        \"\"\"\n",
    "\n",
    "        # Handles missing/NaN values by returning empty string\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "            \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        \n",
    "        # Standardize whitespace\n",
    "        # Normalizes spacing (removes extra spaces)\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        # Standardize common transcription variations\n",
    "        # could be adjusted \n",
    "        # Maps different spellings/variations to standard forms\n",
    "        replacements = {\n",
    "            'uhm': 'um',\n",
    "            'uhh': 'uh',\n",
    "            'hmm': 'hm',\n",
    "            'mhm': 'mm',\n",
    "            'yeah': 'yes',\n",
    "            'yah': 'yes',\n",
    "            'nah': 'no'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = re.sub(r'\\b' + old + r'\\b', new, text)\n",
    "            \n",
    "        return text\n",
    "        \n",
    "    def show_text_cleaning_examples(self, n_examples: int = 5):\n",
    "        \"\"\"Show examples of text cleaning\"\"\"\n",
    "        print(\"\\nText Cleaning Examples:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Whisper examples\n",
    "        print(\"\\nWhisper Transcription Examples:\")\n",
    "        samples = self.whisper_data.sample(n=n_examples)\n",
    "        for _, row in samples.iterrows():\n",
    "            print(f\"Original: {row['original_text']}\")\n",
    "            print(f\"Cleaned : {row['clean_text']}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "        # Human examples\n",
    "        print(\"\\nHuman Transcription Examples:\")\n",
    "        samples = self.human_data.sample(n=n_examples)\n",
    "        for _, row in samples.iterrows():\n",
    "            print(f\"Original: {row['original_text']}\")\n",
    "            print(f\"Cleaned : {row['clean_text']}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "    # Main comparison function\n",
    "    # time_tolerance: allows segments to be slightly offset (0.5 seconds)\n",
    "    # similarity_threshold: minimum text similarity to consider a match (0.3 or 30%)\n",
    "    def find_overlapping_segments(self, time_tolerance: float = 0.5, \n",
    "                                similarity_threshold: float = 0.3):\n",
    "        \"\"\"Find overlapping segments with cleaned text comparison\"\"\"\n",
    "        overlapping = []\n",
    "        \n",
    "        # Filter relevant segments to only those within overlap period\n",
    "        # Reduces processing by excluding non-overlapping parts\n",
    "        human_segments = self.human_data[\n",
    "            (self.human_data['start'] >= self.overlap_start) &\n",
    "            (self.human_data['end'] <= self.overlap_end)\n",
    "        ]\n",
    "\n",
    "        \n",
    "        whisper_segments = self.whisper_data[\n",
    "            (self.whisper_data['start'] >= self.overlap_start) &\n",
    "            (self.whisper_data['end'] <= self.overlap_end)\n",
    "        ]\n",
    "        \n",
    "        print(f\"Processing {len(human_segments)} segments...\")\n",
    "\n",
    "        # Iterates through human segments with progress bar\n",
    "        for _, human_seg in tqdm(human_segments.iterrows()):\n",
    "            # Find potential matches\n",
    "            potential_matches = whisper_segments[\n",
    "                (whisper_segments['start'] >= human_seg['start'] - time_tolerance) &\n",
    "                (whisper_segments['start'] <= human_seg['end'] + time_tolerance)\n",
    "            ]\n",
    "            \n",
    "            for _, whisper_seg in potential_matches.iterrows():\n",
    "                # Calculate similarity using cleaned text\n",
    "                # Returns ratio between 0 (completely different) and 1 (identical)\n",
    "                \"\"\"\n",
    "                Demonstrates SequenceMatcher algorithm with visualization\n",
    "    \n",
    "                Formula for ratio calculation:\n",
    "                ratio = 2.0 * M / (T1 + T2)\n",
    "                where:\n",
    "                - M = sum of length of matching blocks\n",
    "                - T1 = length of text1\n",
    "                - T2 = length of text2\n",
    "                \"\"\"\n",
    "                similarity = SequenceMatcher(\n",
    "                    None,\n",
    "                    human_seg['clean_text'],\n",
    "                    whisper_seg['clean_text']\n",
    "                ).ratio()\n",
    "                \n",
    "                if similarity > similarity_threshold:\n",
    "                    overlapping.append({\n",
    "                        'start_human': human_seg['start'],\n",
    "                        'end_human': human_seg['end'],\n",
    "                        'text_human_original': human_seg['original_text'],\n",
    "                        'text_human_cleaned': human_seg['clean_text'],\n",
    "                        'start_whisper': whisper_seg['start'],\n",
    "                        'end_whisper': whisper_seg['end'],\n",
    "                        'text_whisper_original': whisper_seg['original_text'],\n",
    "                        'text_whisper_cleaned': whisper_seg['clean_text'],\n",
    "                        'text_similarity': similarity,\n",
    "                        'time_diff': abs(human_seg['start'] - whisper_seg['start'])\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(overlapping)\n",
    "\n",
    "    def analyze_overlap(self):\n",
    "        \"\"\"Analyze overlapping segments with detailed text comparison\"\"\"\n",
    "        overlapping = self.find_overlapping_segments()\n",
    "        \n",
    "        stats = {\n",
    "            'overlap_duration': self.overlap_end - self.overlap_start,\n",
    "            'total_duration_whisper': self.whisper_data['end'].max() - self.whisper_data['start'].min(),\n",
    "            'total_duration_human': self.human_data['end'].max() - self.human_data['start'].min(),\n",
    "            'matching_segments': len(overlapping),\n",
    "            'average_similarity': overlapping['text_similarity'].mean() if len(overlapping) > 0 else 0,\n",
    "            'high_similarity_matches': len(overlapping[overlapping['text_similarity'] > 0.7])\n",
    "        }\n",
    "        \n",
    "        return overlapping, stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b36d3-49dd-4c25-ab40-78e693ae0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load your dataframes\n",
    "    whisper_df = pd.read_csv(\"/Users/yueyan/Documents/project/wearable/transcription/025_04_whisper_results_with_speaker.csv\")\n",
    "    human_df = pd.read_csv(\"/Users/yueyan/Documents/project/wearable/transcription/025_04_human_transcription.csv\")\n",
    "    \n",
    "    # Create comparator\n",
    "    comparator = TranscriptionComparator(whisper_df, human_df)\n",
    "    \n",
    "    # Show text cleaning examples\n",
    "    comparator.show_text_cleaning_examples()\n",
    "    \n",
    "    # Analyze overlaps\n",
    "    overlapping, stats = comparator.analyze_overlap()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    \n",
    "    print(f\"\\nProcessing time: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Save results with both original and cleaned text\n",
    "    overlapping.to_csv(\"overlap_comparison_with_text.csv\", index=False)\n",
    "\n",
    "\n",
    "# Create comparator\n",
    "comparator = TranscriptionComparator(whisper_df, human_df)\n",
    "\n",
    "# Show cleaning examples\n",
    "comparator.show_text_cleaning_examples()\n",
    "\n",
    "# Run comparison\n",
    "overlapping, stats = comparator.analyze_overlap()\n",
    "\n",
    "# Check specific examples\n",
    "print(\"\\nExample Comparisons:\")\n",
    "for _, row in overlapping.head().iterrows():\n",
    "    print(\"\\nHuman    :\", row['text_human_original'])\n",
    "    print(\"Cleaned  :\", row['text_human_cleaned'])\n",
    "    print(\"Whisper  :\", row['text_whisper_original'])\n",
    "    print(\"Cleaned  :\", row['text_whisper_cleaned'])\n",
    "    print(f\"Similarity: {row['text_similarity']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "596fc837-c3ec-457c-98ae-a91d0df4ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compare_transcriptions(whisper_file, human_file):\n",
    "    \"\"\"\n",
    "    Compare whisper and human transcriptions and display aligned results\n",
    "    \n",
    "    Parameters:\n",
    "    whisper_file: DataFrame with columns ['start', 'end', 'text']\n",
    "    human_file: DataFrame with columns ['start', 'end', 'text']\n",
    "    \"\"\"\n",
    "    # Initialize the comparator\n",
    "    comparator = TranscriptionComparator(whisper_file, human_file)\n",
    "    \n",
    "    # Get overlapping segments and stats\n",
    "    overlapping_segments, stats = comparator.analyze_overlap()\n",
    "    \n",
    "    # Sort by human start time for chronological display\n",
    "    overlapping_segments = overlapping_segments.sort_values('start_human')\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nTranscription Comparison Statistics:\")\n",
    "    print(f\"Total overlap duration: {stats['overlap_duration']:.2f} seconds\")\n",
    "    print(f\"Number of matching segments: {stats['matching_segments']}\")\n",
    "    print(f\"Average text similarity: {stats['average_similarity']:.2%}\")\n",
    "    print(f\"High similarity matches (>70%): {stats['high_similarity_matches']}\")\n",
    "    \n",
    "    # Display aligned transcriptions\n",
    "    print(\"\\nAligned Transcriptions:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Time (s)':<15} {'Human Transcription':<40} {'Whisper Transcription':<40} {'Similarity'}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for _, row in overlapping_segments.iterrows():\n",
    "        time_range = f\"{row['start_human']:.1f}-{row['end_human']:.1f}\"\n",
    "        print(f\"{time_range:<15} {row['text_human_original']:<40} {row['text_whisper_original']:<40} {row['text_similarity']:.2%}\")\n",
    "\n",
    "    return overlapping_segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665b1dd5-8f45-4595-a038-40e0ad019124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and cleaned. Time range: 79.20s - 373.53s\n",
      "Processing 166 segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [00:00, 3291.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription Comparison Statistics:\n",
      "Total overlap duration: 294.33 seconds\n",
      "Number of matching segments: 188\n",
      "Average text similarity: 79.35%\n",
      "High similarity matches (>70%): 134\n",
      "\n",
      "Aligned Transcriptions:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time (s)        Human Transcription                      Whisper Transcription                    Similarity\n",
      "----------------------------------------------------------------------------------------------------\n",
      "79.5-82.6       you wanna look at the caterpillar? i see you looking at the doggy over there. I see you looking at the doggie over there. 68.38%\n",
      "83.0-83.4       yeah.                                    Yeah.                                    100.00%\n",
      "84.3-84.8       look at that.                            Look at that.                            100.00%\n",
      "84.3-84.8       look at that.                            Look at that.                            100.00%\n",
      "85.6-86.1       look at that.                            Look at that.                            100.00%\n",
      "86.6-87.7       goes around in a circle.                 It goes around in a circle.              93.88%\n",
      "88.9-90.1       goes around in a circle.                 It goes around in a circle.              93.88%\n",
      "90.8-91.7       yes it does.                             Yes, it does.                            100.00%\n",
      "92.9-94.1       it makes noise.                          It makes noise.                          100.00%\n",
      "95.7-97.4       it makes noise look at that.             Look at that.                            61.54%\n",
      "95.7-97.4       it makes noise look at that.             Oh, and it tastes so good.               43.14%\n",
      "95.7-97.4       it makes noise look at that.             It makes noise.                          68.29%\n",
      "97.7-99.5       oh and it tastes good!                   Oh, and it tastes so good.               93.33%\n",
      "97.7-99.5       oh and it tastes good!                   It tastes so good.                       73.68%\n",
      "99.5-99.5       oh and it tastes good!                   It tastes so good.                       73.68%\n",
      "100.1-101.5     it tastes so good.                       It tastes so good.                       100.00%\n",
      "103.9-105.7     hello Evy my name is caterpillar.        Hello, Eve.                              43.90%\n",
      "103.9-105.7     hello Evy my name is caterpillar.        My name is Caterpillar.                  81.48%\n",
      "106.5-107.3     cate-caterpillar.                        Cat the caterpillar.                     88.24%\n",
      "108.7-109.6     hello hello hello.                       Hello, hello, hello.                     100.00%\n",
      "111.0-112.0     you looking at his face?                 You looking at his face?                 100.00%\n",
      "112.9-113.9     you looking at his face?                 You looking at his face?                 100.00%\n",
      "115.2-116.6     hello Evy.                               Hello.                                   71.43%\n",
      "115.2-116.6     hello Evy.                               Hello, Eve.                              88.89%\n",
      "117.0-117.9     hello.                                   Hello.                                   100.00%\n",
      "118.6-119.5     i have a green face!                     I have a green face.                     100.00%\n",
      "120.2-121.4     and blue antenna!                        I have a blue antenna.                   75.68%\n",
      "120.2-121.4     and blue antenna!                        Green face.                              30.77%\n",
      "122.2-122.9     green face.                              Green face.                              100.00%\n",
      "123.4-124.5     and blue antenna                         I have a blue antenna.                   75.68%\n",
      "125.5-126.0     wanna get it?                            Want to get it?                          76.92%\n",
      "127.0-128.0     good job.                                Good job.                                100.00%\n",
      "128.6-129.7     good job.                                Good job.                                100.00%\n",
      "133.2-134.4     you wanna play with this one?            I can see you looking at this dog.       42.62%\n",
      "133.2-134.4     you wanna play with this one?            You want to play with this one?          89.66%\n",
      "134.7-136.9     i could see you looking at this dog.     I can see you looking at this dog.       91.18%\n",
      "134.7-136.9     i could see you looking at this dog.     Look at the doggie.                      52.83%\n",
      "137.6-138.7     look at the doggy.                       Look at the doggie.                      91.43%\n",
      "137.6-138.7     look at the doggy.                       Look at the doggie.                      91.43%\n",
      "139.3-141.5     look at the doggy. can you see the little doggy? Look at the doggie.                      53.12%\n",
      "139.3-141.5     look at the doggy. can you see the little doggy? Can you see the little doggie?           72.00%\n",
      "142.3-142.9     can you grab it?                         Can you grab it?                         100.00%\n",
      "144.2-145.2     he rolled around.                        He rolls around.                         90.32%\n",
      "144.2-145.2     he rolled around.                        He rolls around.                         90.32%\n",
      "145.8-147.6     he rolled around oh!                     He rolls around.                         82.35%\n",
      "150.3-151.8     got a little blue hat on.                He's got a little blue hat on.           92.31%\n",
      "152.3-154.2     and he rolls around on his bum.          He rolls around on his bum.              92.86%\n",
      "154.6-155.1     you see that?                            Can you see that?                        85.71%\n",
      "156.4-157.0     you see that?                            See that?                                80.00%\n",
      "156.4-157.0     you see that?                            Look at that.                            58.33%\n",
      "157.4-158.0     look at that.                            Look at that.                            100.00%\n",
      "159.5-159.9     huh?                                     Uh-huh.                                  75.00%\n",
      "163.3-164.6     you can touch that oops.                 Look at the tiny hat.                    41.86%\n",
      "163.3-164.6     you can touch that oops.                 Oh, you're good at grabbing.             33.33%\n",
      "167.4-168.4     you're good at grabbing.                 You're good at grabbing.                 100.00%\n",
      "169.5-170.2     looking at oh!                           Look at him.                             75.00%\n",
      "169.5-170.2     looking at oh!                           Oh, tilting around.                      46.67%\n",
      "170.6-171.8     tilting around.                          Going back and forth.                    41.18%\n",
      "172.2-173.4     go back and forth.                       Going back and forth.                    91.89%\n",
      "174.3-175.4     back and forth.                          Going back and forth.                    82.35%\n",
      "176.5-177.5     back and forth.                          Going back and forth.                    82.35%\n",
      "178.0-178.6     hey Evy.                                 Hey, Evie.                               80.00%\n",
      "179.2-180.4     look at the lion over there.             Look at the lion over there.             100.00%\n",
      "182.0-182.5     Evy.                                     Evie.                                    57.14%\n",
      "183.3-184.6     look at the lion over there.             Look at the lion over there.             100.00%\n",
      "185.3-185.8     hey Evy.                                 Hey, Evie.                               80.00%\n",
      "187.2-188.5     look at the lion over there.             Look at the lion over there.             100.00%\n",
      "189.4-190.4     don't wanna look at the lion.            Come on, look at the lion.               74.51%\n",
      "191.3-191.9     hey Evy.                                 Hey, Evie.                               80.00%\n",
      "192.5-193.1     look at that!                            Look at the car.                         81.48%\n",
      "192.5-193.1     look at that!                            Look at that.                            100.00%\n",
      "193.7-194.5     look at the car.                         Look at the car.                         100.00%\n",
      "195.9-197.7     wanna see the car? look at the car.      Want to see the car?                     57.69%\n",
      "195.9-197.7     wanna see the car? look at the car.      Look at the car.                         62.50%\n",
      "198.7-201.0     no you wanna play with this one cause it's right in front of you. No, you want to play with this one       60.42%\n",
      "198.7-201.0     no you wanna play with this one cause it's right in front of you. because it's right in front of you.      66.67%\n",
      "201.7-202.7     he's right in front of you.              It's right in front of you.              92.00%\n",
      "203.6-203.8     yeah.                                    Yeah?                                    100.00%\n",
      "205.1-206.1     yeah look at that.                       Yeah, look at that.                      100.00%\n",
      "207.2-208.0     look at that.                            Look at that.                            100.00%\n",
      "209.3-211.3     do you see another toy over here?        Look at this one, Evie.                  30.19%\n",
      "212.2-213.5     look at this one Evy.                    Look at this one, Evie.                  92.68%\n",
      "214.9-216.2     that one.                                That one.                                100.00%\n",
      "217.2-217.4     it's got.                                It's got buttons on it.                  50.00%\n",
      "220.0-221.2     it's got buttons.                        It's got buttons.                        100.00%\n",
      "222.2-222.8     yeah.                                    Yeah.                                    100.00%\n",
      "223.6-224.5     you don't like that one.                 You don't like that one.                 100.00%\n",
      "225.3-228.0     oh you don't like that one. i think you like the caterpillar one the best. No, you don't like that one.             50.00%\n",
      "225.3-228.0     oh you don't like that one. i think you like the caterpillar one the best. I think you like the caterpillar one the best. 77.59%\n",
      "239.3-239.9     hey Evy.                                 Hey, Evie.                               80.00%\n",
      "239.3-239.9     hey Evy.                                 Evie.                                    36.36%\n",
      "240.3-240.7     Evy.                                     Evie.                                    57.14%\n",
      "241.2-242.1     look at the lion.                        Look at the lion.                        100.00%\n",
      "242.7-244.8     hey Evy can you see the lion over there? Can you see the lion over there?         88.57%\n",
      "245.6-248.1     you can't really see the lion you're kind of short huh? You can't really see the lion.           70.00%\n",
      "245.6-248.1     you can't really see the lion you're kind of short huh? You're kind of short, huh?               61.33%\n",
      "248.9-250.0     you're kind of short arentcha?           You're kind of short, aren't you?        87.72%\n",
      "250.5-251.1     hey Evy.                                 Hey, Evie.                               80.00%\n",
      "251.4-252.4     can i have my hand back please?          Can I have my hand back, please?         100.00%\n",
      "253.2-253.9     look at the pig!                         Look at the pig.                         100.00%\n",
      "254.6-255.4     can you see the pig?                     Can you see the pig?                     100.00%\n",
      "256.8-257.4     hey Evy.                                 Hey, Evie.                               80.00%\n",
      "258.1-258.9     look at the pig.                         Look at the pig.                         100.00%\n",
      "259.6-260.6     it goes oink oink oink.                  Oink, oink, oink.                        77.78%\n",
      "259.6-260.6     it goes oink oink oink.                  It goes oink, oink, oink.                100.00%\n",
      "260.6-260.7     it goes oink oink oink.                  Oink, oink, oink.                        77.78%\n",
      "263.7-264.4     hey Evy.                                 Hey, Evie.                               80.00%\n",
      "265.2-265.7     Evy.                                     Evie.                                    57.14%\n",
      "266.5-269.5     you wanna look at the dog? the dalmatian dog? You want to look at the dog?             65.71%\n",
      "266.5-269.5     you wanna look at the dog? the dalmatian dog? The Dalmatian dog?                       56.67%\n",
      "266.5-269.5     you wanna look at the dog? the dalmatian dog? You can see that one, can't you?         47.22%\n",
      "269.5-269.5     you wanna look at the dog? the dalmatian dog? You can see that one, can't you?         47.22%\n",
      "270.1-271.8     you can see that one can't you?          You can see that one, can't you?         100.00%\n",
      "272.4-273.3     he's black and white.                    He's got spots.                          37.50%\n",
      "272.4-273.3     he's black and white.                    He's black and white.                    100.00%\n",
      "273.5-274.4     got spots.                               He's got spots.                          81.82%\n",
      "275.5-277.3     you're like what is this thing i'm sitting in? You're like, what is this thing I'm sitting in? 100.00%\n",
      "278.4-278.8     yeah.                                    Yeah.                                    100.00%\n",
      "280.2-280.6     yeah.                                    Yeah.                                    100.00%\n",
      "281.3-282.9     oh see now you're looking around.        Oh, see, now you're looking around.      100.00%\n",
      "281.3-282.9     oh see now you're looking around.        Now you're looking around.               87.27%\n",
      "283.4-284.6     now you're looking around.               Now you're looking around.               100.00%\n",
      "285.3-285.7     yeah.                                    Yeah.                                    100.00%\n",
      "286.3-287.6     hey Evy can i have my fingers back?      Hey, Evie.                               38.10%\n",
      "286.3-287.6     hey Evy can i have my fingers back?      Can I have my fingers back?              86.67%\n",
      "286.3-287.6     hey Evy can i have my fingers back?      Can I have my fingers back?              86.67%\n",
      "288.2-289.1     can i have my fingers back?              Can I have my fingers back?              100.00%\n",
      "288.2-289.1     can i have my fingers back?              Can I have my fingers back?              100.00%\n",
      "289.1-289.1     can i have my fingers back?              Can I have my fingers back?              100.00%\n",
      "289.5-290.5     can i have my fingers back?              Can I have my fingers back?              100.00%\n",
      "289.5-290.5     can i have my fingers back?              Want to look at the pig?                 36.73%\n",
      "290.5-290.5     can i have my fingers back?              Want to look at the pig?                 36.73%\n",
      "292.7-293.6     wanna look at the pig?                   Want to look at the pig?                 86.36%\n",
      "294.5-295.2     look at the pig!                         Look at the pig.                         100.00%\n",
      "296.7-298.5     you're looking at my finger, you're not looking at the pig. You're looking at my finger.             64.20%\n",
      "296.7-298.5     you're looking at my finger, you're not looking at the pig. You're not looking at the pig.           67.47%\n",
      "296.7-298.5     you're looking at my finger, you're not looking at the pig. You're looking at my finger.             64.20%\n",
      "298.9-299.9     you're looking at my finger.             You're looking at my finger.             100.00%\n",
      "301.0-302.2     you wanna look at the lion?              You want to look at the lion?            88.89%\n",
      "301.0-302.2     you wanna look at the lion?              The lion over there.                     35.56%\n",
      "302.2-302.2     you wanna look at the lion?              The lion over there.                     35.56%\n",
      "303.0-304.1     it's a lion over there.                  The lion over there.                     85.00%\n",
      "304.6-305.9     oh you're just looking at my finger.     Now you're just looking at my finger.    95.65%\n",
      "306.8-308.0     looking at my finger again.              You're looking at my finger again.       89.66%\n",
      "308.5-308.9     yeah!                                    Yeah.                                    100.00%\n",
      "309.8-310.6     ok Evy.                                  Okay, Evie.                              66.67%\n",
      "311.3-311.7     Evy.                                     Evie.                                    57.14%\n",
      "313.2-313.7     Evy.                                     Evie.                                    57.14%\n",
      "314.3-315.3     wanna look at the piggy?                 Want to look at the piggy?               87.50%\n",
      "314.3-315.3     wanna look at the piggy?                 You see the pig?                         47.37%\n",
      "315.9-316.5     you see the pig?                         You see the pig?                         100.00%\n",
      "315.9-316.5     you see the pig?                         He's pink.                               43.48%\n",
      "316.5-316.5     you see the pig?                         He's pink.                               43.48%\n",
      "316.9-317.6     he's pink!                               He's pink.                               100.00%\n",
      "318.7-319.5     the pink pig.                            The pink pig.                            100.00%\n",
      "322.1-323.9     i know. what are you sitting in huh?     What are you sitting in, huh?            88.52%\n",
      "322.1-323.9     i know. what are you sitting in huh?     It's your favorite color.                35.09%\n",
      "324.7-326.7     it's your favorite color. it's orange.   It's your favorite color.                80.70%\n",
      "324.7-326.7     it's your favorite color. it's orange.   It's orange.                             45.45%\n",
      "327.4-328.3     hey look Evy.                            Hey, look, Evie.                         88.00%\n",
      "327.4-328.3     hey look Evy.                            Want to look at the lion?                38.89%\n",
      "328.8-329.8     wanna look at the lion?                  Want to look at the lion?                86.96%\n",
      "330.8-331.7     no i wanna eat the tray.                 No, I want to eat the tray.              87.50%\n",
      "330.8-331.7     no i wanna eat the tray.                 Look at the lion.                        46.15%\n",
      "332.2-333.2     look at the lion?                        No, I want to eat the tray.              48.78%\n",
      "332.2-333.2     look at the lion?                        Look at the lion.                        100.00%\n",
      "333.8-334.8     no i wanna eat the tray.                 No, I want to eat the tray.              87.50%\n",
      "335.9-336.8     you silly girl.                          You silly girl.                          100.00%\n",
      "337.7-338.4     hey Evy.                                 Hey, Evie.                               80.00%\n",
      "339.0-339.6     hey Sweetie.                             Hey, Evie.                               73.68%\n",
      "340.7-341.1     hi.                                      Hi.                                      100.00%\n",
      "347.3-349.7     no you don't wanna look at the dog.      Don't look at the dog.                   75.47%\n",
      "350.3-351.5     you wanna look at the dog?               Don't look at the dog.                   80.00%\n",
      "352.4-354.2     there he is. he's a dalmatian dog.       There he is.                             52.38%\n",
      "352.4-354.2     there he is. he's a dalmatian dog.       He's a Dalmatian dog.                    76.00%\n",
      "352.4-354.2     there he is. he's a dalmatian dog.       With a red hat on.                       37.50%\n",
      "354.2-354.2     there he is. he's a dalmatian dog.       With a red hat on.                       37.50%\n",
      "354.8-355.8     with a red hat on.                       With a red hat on.                       100.00%\n",
      "356.7-357.8     like fire trucks.                        Like fire trucks.                        100.00%\n",
      "358.8-359.0     yeah.                                    Yeah.                                    100.00%\n",
      "360.4-360.4     yeah. oh are you getting tired?          Oh, are you getting tired?               92.31%\n",
      "360.4-360.6     yeah. oh are you getting tired?          Oh, are you getting tired?               92.31%\n",
      "360.6-363.0     yeah. oh are you getting tired?          Oh, are you getting tired?               92.31%\n",
      "363.7-364.7     you wanna get out?                       You want to get out?                     83.33%\n",
      "367.0-368.8     wanna look at the lion?                  Want to look at the lion?                86.96%\n",
      "367.0-368.8     wanna look at the lion?                  Want to look at the lion?                86.96%\n",
      "369.1-370.3     wanna look at the lion?                  Want to look at the lion?                86.96%\n",
      "371.8-373.5     no you're scrunched all down there.      He's cringed all down there.             71.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "whisper_df = pd.read_csv(\"/Users/yueyan/Documents/project/wearable/transcription/025_04_whisper_results_with_speaker.csv\")\n",
    "human_df = pd.read_csv(\"/Users/yueyan/Documents/project/wearable/transcription/025_04_human_transcription.csv\")\n",
    "aligned_segments = compare_transcriptions(whisper_df, human_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a0c0f-54ea-4647-8355-fff69afd0362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
