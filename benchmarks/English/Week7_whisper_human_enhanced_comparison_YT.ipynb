{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bc4508-c69d-4bfa-9557-76082b16c03b",
   "metadata": {},
   "source": [
    "# Parent-Infant Interaction Analysis Pipeline\n",
    "\n",
    "## Setup and Dependencies\n",
    "```python\n",
    "import torch\n",
    "import whisper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "from scipy.signal import butter, filtfilt\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import soundfile as sf\n",
    "from typing import Union, List, Tuple\n",
    "```\n",
    "Key dependencies:\n",
    "- OpenAI Whisper for speech recognition\n",
    "- PyDub for audio processing\n",
    "- FFmpeg for audio file manipulation\n",
    "- Pandas for data handling\n",
    "- NumPy for numerical operations\n",
    "\n",
    "## Pipeline Overview\n",
    "This notebook implements a three-stage pipeline for analyzing parent-infant interactions:\n",
    "1. Audio Preprocessing\n",
    "2. Speech Recognition\n",
    "3. Accuracy Analysis\n",
    "\n",
    "### 1. Audio Preprocessing (`AudioPreprocessor` class)\n",
    "Prepares audio files for optimal speech recognition:\n",
    "- Converts stereo to mono (required for Whisper)\n",
    "- Adjusts sample rate to 16kHz\n",
    "- Normalizes volume\n",
    "- Optional silence removal (not recommended for interaction analysis)\n",
    "\n",
    "Example usage:\n",
    "```python\n",
    "preprocessor = AudioPreprocessor(input_file)\n",
    "processed_file = preprocessor.process_audio(\n",
    "    output_dir=output_dir,\n",
    "    remove_silence=False  # Keep natural pauses\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. Speech Recognition (`transcribe_audio` function)\n",
    "Uses Whisper ASR with parent-infant specific settings:\n",
    "- Large-v2 model for accuracy\n",
    "- Custom prompt for infant vocalizations\n",
    "- Word-level timestamps\n",
    "- Speaker detection capabilities\n",
    "\n",
    "Output includes:\n",
    "- Transcribed text\n",
    "- Timing information\n",
    "- Optional speaker labels\n",
    "\n",
    "### 3. Accuracy Analysis (`EnhancedTranscriptionComparator` class)\n",
    "Compares Whisper output with human transcriptions:\n",
    "- Text similarity scoring\n",
    "- Word-level accuracy metrics\n",
    "- Detailed analysis reports\n",
    "\n",
    "### Key Metrics\n",
    "1. Text Similarity\n",
    "   - SequenceMatcher score (Ratcliff/Obershelp algorithm)\n",
    "   - Formula: ratio = 2.0 * M / T\n",
    "     * M = sum of lengths of matched subsequences\n",
    "     * T = total length of both strings combined\n",
    "\n",
    "2. Word-level Analysis\n",
    "   - Word count comparison\n",
    "   - Correct words identification\n",
    "   - Mismatch detection\n",
    "   - Accuracy percentage calculation\n",
    "\n",
    "### Output Format\n",
    "CSV file with columns:\n",
    "- Time Range\n",
    "- Whisper Transcription\n",
    "- Human Transcription\n",
    "- SequenceMatcher Score\n",
    "- Word Count (Human)\n",
    "- Word Count (Whisper)\n",
    "- Correct Words\n",
    "- Mismatches\n",
    "- Accuracy (%)\n",
    "- Comments/Notes\n",
    "\n",
    "## Results Interpretation\n",
    "The comparison output provides:\n",
    "- Time-aligned transcriptions\n",
    "- Word-level accuracy metrics\n",
    "- Quality indicators\n",
    "- Automated issue detection\n",
    "\n",
    "Example metrics from current run:\n",
    "- Total segments: 95\n",
    "- Average accuracy: 65.3%\n",
    "- High quality matches: 39\n",
    "- Low quality matches: 38\n",
    "\n",
    "## Future Improvements\n",
    "Consider:\n",
    "- GPU acceleration for faster processing\n",
    "- Enhanced speaker detection\n",
    "- Controlled vocabulary for infant sounds\n",
    "- Batch processing capabilities\n",
    "\n",
    "## Suggested File Structure\n",
    "\n",
    "```\n",
    "File Structure Overview:\n",
    "project/wearable/\n",
    "├── media/[MONTH]/                    # Audio files\n",
    "│   ├── IW_[ID]_[MONTH]_TL.wav       # Original audio\n",
    "│   └── processed/                    # Processed audio files\n",
    "│       ├── IW_[ID]_[MONTH]_TL_mono.wav\n",
    "│       ├── IW_[ID]_[MONTH]_TL_mono_16k.wav\n",
    "│       └── IW_[ID]_[MONTH]_TL_mono_16k_normalized.wav\n",
    "│\n",
    "└── transcription/[MONTH]/            # Transcription files\n",
    "    ├── IW_[ID]_[MONTH]_whisper_results_without_speaker.csv  # Whisper output\n",
    "    └── IW_[ID]_[MONTH]_transcription_comparison.csv         # Analysis results\n",
    "\n",
    "Naming Convention:\n",
    "- [ID]: Participant ID (e.g., 002)\n",
    "- [MONTH]: Recording month (e.g., 12mon)\n",
    "```\n",
    "\n",
    "## Usage Workflow\n",
    "\n",
    "### 1. Preprocess Audio\n",
    "```python\n",
    "# Initialize and process audio\n",
    "preprocessor = AudioPreprocessor(\"/path/to/media/12mon/IW_002_12_TL.wav\")\n",
    "processed_file = preprocessor.process_audio(\n",
    "    output_dir=\"/path/to/media/12mon/processed\",\n",
    "    remove_silence=False    # Keep silence for interaction analysis\n",
    ")\n",
    "# Output: IW_002_12_TL_mono_16k_normalized.wav\n",
    "```\n",
    "\n",
    "### 2. Transcribe Audio\n",
    "```python\n",
    "# Run Whisper transcription\n",
    "df, result = transcribe_audio(\n",
    "    audio_path=\"/path/to/processed/IW_002_12_TL_mono_16k_normalized.wav\",\n",
    "    speaker_hints=[\"[Mother]\", \"[Father]\", \"[Baby]\"]\n",
    ")\n",
    "df.to_csv(\"002_12_whisper_results.csv\", index=False)\n",
    "```\n",
    "\n",
    "### 3. Compare Transcriptions\n",
    "```python\n",
    "# Compare with human transcription\n",
    "results_df, stats = compare_and_export(\n",
    "    whisper_df,    # Whisper output\n",
    "    human_df,      # Human transcription\n",
    "    \"002_12_transcription_comparison.csv\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Expected Outputs\n",
    "1. Processed Audio: `*_mono_16k_normalized.wav`\n",
    "2. Whisper Output: `*_whisper_results.csv`\n",
    "3. Analysis: `*_transcription_comparison.csv`\n",
    "\n",
    "\n",
    "## Technical Notes\n",
    "- Time tolerance: 0.5 seconds for segment matching\n",
    "- Similarity threshold: 0.3 for considering matches\n",
    "- Text cleaning includes:\n",
    "  * Case normalization\n",
    "  * Punctuation removal\n",
    "  * Common transcription variant standardization\n",
    "- High quality matches: Accuracy > 80%\n",
    "- Low quality matches: Accuracy < 60%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193fc90b-898a-4525-8bac-8ec64cc61401",
   "metadata": {},
   "outputs": [],
   "source": [
    "###preprocessing\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, input_path: Union[str, Path]):\n",
    "        \"\"\"Initialize audio preprocessor with input file path\"\"\"\n",
    "        self.input_path = Path(input_path)\n",
    "        self.setup_logging()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging configuration\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def process_audio(self, \n",
    "                     output_dir: Union[str, Path],\n",
    "                     apply_noise_reduction: bool = True,\n",
    "                     noise_reduction_strength: float = 0.21,\n",
    "                     target_volume: float = 1.5) -> Path:\n",
    "        \"\"\"\n",
    "        Process audio using FFmpeg filter chain:\n",
    "        1. Convert to mono\n",
    "        2. Adjust sample rate to 16kHz\n",
    "        3. Optional noise reduction\n",
    "        4. Normalize volume\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save processed audio\n",
    "            apply_noise_reduction: Whether to apply noise reduction\n",
    "            noise_reduction_strength: Strength of noise reduction (0.01 to 1.0)\n",
    "            target_volume: Target volume for normalization\n",
    "            \n",
    "        Returns:\n",
    "            Path to processed audio file\n",
    "        \"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Construct output path\n",
    "        output_path = output_dir / f\"{self.input_path.stem}_processed.wav\"\n",
    "        \n",
    "        # Build FFmpeg filter chain\n",
    "        filters = [\n",
    "            'aresample=16000',  # Convert to 16kHz\n",
    "            'aformat=sample_fmts=s16:channel_layouts=mono',  # Convert to mono\n",
    "        ]\n",
    "        \n",
    "        # Add noise reduction if requested\n",
    "        if apply_noise_reduction:\n",
    "            filters.append(f'anlmdn=s={noise_reduction_strength}')\n",
    "        \n",
    "        # Add volume normalization\n",
    "        filters.append(f'volume={target_volume}')\n",
    "        \n",
    "        # Join filters into a single filter chain\n",
    "        filter_chain = ','.join(filters)\n",
    "        \n",
    "        # Build FFmpeg command\n",
    "        command = [\n",
    "            'ffmpeg',\n",
    "            '-i', str(self.input_path),\n",
    "            '-af', filter_chain,\n",
    "            '-ar', '16000',  # Ensure 16kHz output\n",
    "            '-ac', '1',      # Ensure mono output\n",
    "            '-y',            # Overwrite output if exists\n",
    "            str(output_path)\n",
    "        ]\n",
    "        \n",
    "        # Log the command for debugging\n",
    "        self.logger.info(\"Running FFmpeg command:\")\n",
    "        self.logger.info(' '.join(command))\n",
    "        \n",
    "        try:\n",
    "            # Run FFmpeg command\n",
    "            subprocess.run(command, \n",
    "                         check=True,\n",
    "                         capture_output=True,\n",
    "                         text=True)\n",
    "            \n",
    "            self.logger.info(f\"Successfully processed audio to: {output_path}\")\n",
    "            return output_path\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.logger.error(f\"FFmpeg error: {e.stderr}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during processing: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = Path(\"/Users/yueyan/Documents/project/wearable/media/12mon/raw/IW_002_12_TL.wav\")\n",
    "    output_dir = Path(\"/Users/yueyan/Documents/project/wearable/media/12mon/processed\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize preprocessor\n",
    "        preprocessor = AudioPreprocessor(input_file)\n",
    "        \n",
    "        # Process audio\n",
    "        processed_file = preprocessor.process_audio(\n",
    "            output_dir=output_dir,\n",
    "            apply_noise_reduction=True,\n",
    "            noise_reduction_strength=0.01,  # Adjust between 0.01 and 1.0\n",
    "            target_volume=1.5              # Adjust as needed\n",
    "        )\n",
    "        \n",
    "        print(f\"Processed file: {processed_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Processing failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363563b4-b21f-4b50-b631-9f81bfe43a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper transciption with speaker recognition\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "\n",
    "def transcribe_audio(audio_path, speaker_hints=None):\n",
    "    \"\"\"Transcribe audio using Whisper with enhanced speaker detection\"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    model = whisper.load_model(\"large-v2\", device=\"cpu\")\n",
    "    \n",
    "    # Keep the parent-infant context in the prompt but make it more transcription-focused\n",
    "    initial_prompt = \"\"\"This is a parent-infant interaction recording. \n",
    "    Please transcribe all speech, including infant vocalizations, parent speech, and any notable sounds.\n",
    "    Include both verbal and non-verbal vocalizations.\"\"\"\n",
    "    \n",
    "    print(\"Transcribing...\")\n",
    "    result = model.transcribe(\n",
    "        audio_path,\n",
    "        word_timestamps=True,\n",
    "        verbose=True,\n",
    "        initial_prompt=initial_prompt,\n",
    "        language=\"en\",\n",
    "        task=\"transcribe\"\n",
    "    )\n",
    "    \n",
    "    # Process segments\n",
    "    segments = []\n",
    "    \n",
    "    # First pass: Get clean transcriptions with timing\n",
    "    for segment in result[\"segments\"]:\n",
    "        text = segment[\"text\"].strip()\n",
    "        start = segment[\"start\"]\n",
    "        end = segment[\"end\"]\n",
    "        duration = end - start\n",
    "        \n",
    "        # Skip empty segments\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        segment_info = {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'start_time': str(timedelta(seconds=int(start))),\n",
    "            'end_time': str(timedelta(seconds=int(end))),\n",
    "            'text': text,\n",
    "            'duration': duration\n",
    "        }\n",
    "        \n",
    "        # Optional: Add speaker detection without modifying the original text\n",
    "        if speaker_hints:\n",
    "            speaker = detect_speaker(text, speaker_hints)\n",
    "            segment_info['speaker'] = speaker\n",
    "        \n",
    "        segments.append(segment_info)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(segments)\n",
    "    \n",
    "    return df, result\n",
    "\n",
    "def detect_speaker(text, speaker_hints):\n",
    "    \"\"\"Separate function for speaker detection that doesn't modify the transcription\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Basic speaker detection logic\n",
    "    if any(word in text_lower for word in ['crying', 'cries', 'waa']):\n",
    "        return '[Baby Crying]'\n",
    "    elif any(word in text_lower for word in ['laugh', 'giggle']):\n",
    "        return '[Baby Laughing]'\n",
    "    elif any(word in text_lower for word in ['goo', 'gah', 'bah', 'coo']):\n",
    "        return '[Infant Vocalization]'\n",
    "    elif '[mother]' in text_lower or 'mom' in text_lower:\n",
    "        return '[Mother]'\n",
    "    elif '[father]' in text_lower or 'dad' in text_lower:\n",
    "        return '[Father]'\n",
    "    else:\n",
    "        return '[Unspecified]'\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    speaker_hints = [\n",
    "        \"[Mother]\", \"[Father]\", \"[Baby]\",\n",
    "        \"[Infant Vocalization]\", \"[Baby Crying]\", \"[Baby Laughing]\"\n",
    "    ]\n",
    "\n",
    "    ###replace with your audio file\n",
    "    try:\n",
    "        df, result = transcribe_audio(\n",
    "            \"/Users/yueyan/Documents/project/wearable/media/12mon/your file name\",\n",
    "            speaker_hints=speaker_hints\n",
    "        )\n",
    "        \n",
    "        # Save results; replace with your path\n",
    "        df.to_csv(\"/Users/yueyan/Documents/project/wearable/transcription/12mon/your file name to be saved\", index=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nTranscription Statistics:\")\n",
    "        print(f\"Total segments: {len(df)}\")\n",
    "        \n",
    "        # If speaker detection is enabled\n",
    "        if 'speaker' in df.columns:\n",
    "            print(\"\\nSpeaker distribution:\")\n",
    "            print(df['speaker'].value_counts())\n",
    "        \n",
    "        print(\"\\nFirst few transcriptions:\")\n",
    "        print(df[['start_time', 'end_time', 'text']].head())\n",
    "        \n",
    "        # Optionally display with speakers if available\n",
    "        if 'speaker' in df.columns:\n",
    "            print(\"\\nFirst few transcriptions with speakers:\")\n",
    "            print(df[['start_time', 'end_time', 'speaker', 'text']].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076d8961-e6c8-469d-ba0b-697ff11785eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yueyan/.pyenv/versions/3.9.7/lib/python3.9/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yueyan/.pyenv/versions/3.9.7/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.880 --> 00:02.560]  It's just Dougie, so...\n",
      "[00:02.560 --> 00:02.980]  Come here, you.\n",
      "[00:03.400 --> 00:04.640]  ...if you don't mind watching him play.\n",
      "[00:06.100 --> 00:08.080]  Oh no, look what I do!\n",
      "[00:13.220 --> 00:15.420]  The camera is really fast.\n",
      "[00:22.800 --> 00:27.660]  Your play area is about where your leg is, so you guys have all that room to play.\n",
      "[00:28.080 --> 00:31.000]  Okay, and I'll be back in four minutes to pick up the toys.\n",
      "[00:33.760 --> 00:35.460]  Dougie, are you just going to eat the block?\n",
      "[00:37.360 --> 00:38.160]  Oh, squeaky!\n",
      "[00:43.820 --> 00:44.220]  Dougie...\n",
      "[00:46.400 --> 00:47.780]  Oh, did it tickle?\n",
      "[00:48.240 --> 00:51.080]  Did it tickle? Here, got it? Oh, you got it!\n",
      "[00:51.980 --> 00:52.360]  Come on.\n",
      "[00:53.460 --> 00:53.860]  Look.\n",
      "[01:01.040 --> 01:01.780]  Cool.\n",
      "[01:12.220 --> 01:15.720]  Oh, you got spit on this one already.\n",
      "[01:17.260 --> 01:18.260]  What is he doing?\n",
      "[01:21.780 --> 01:23.520]  What is that?\n",
      "[01:25.040 --> 01:27.100]  Yellow. Yellow block.\n",
      "[01:28.020 --> 01:29.620]  Yellow block.\n",
      "[01:29.920 --> 01:32.800]  Look, it's got a seven and a duck.\n",
      "[01:33.580 --> 01:35.180]  Duckies. Duckies.\n",
      "[01:37.100 --> 01:37.520]  It's mine.\n",
      "[01:38.940 --> 01:39.840]  Yeah?\n",
      "[01:44.960 --> 01:46.400]  Red box.\n",
      "[01:47.540 --> 01:50.780]  Look at it. There's kitties on that one.\n",
      "[01:53.320 --> 01:54.140]  Kitties.\n",
      "[02:08.440 --> 02:10.080]  Good one!\n",
      "[02:10.440 --> 02:11.720]  You're touching it with your foot.\n",
      "[02:20.780 --> 02:22.500]  You just want to eat it?\n",
      "[02:24.020 --> 02:29.080]  I like the kitties. Look, there's a mommy kitty and baby kitties.\n",
      "[02:31.720 --> 02:32.920]  You see the bears?\n",
      "[02:35.260 --> 02:35.820]  Bear.\n",
      "[02:37.600 --> 02:38.200]  Bears.\n",
      "[02:45.580 --> 02:47.760]  Yeah, good one. You got it all.\n",
      "[02:50.780 --> 02:51.520]  Look.\n",
      "[03:00.780 --> 03:02.800]  Look it. There's a ducky.\n",
      "[03:03.780 --> 03:04.680]  Can you say ducky?\n",
      "[03:06.820 --> 03:09.020]  Good! Can you say ducky?\n",
      "[03:10.020 --> 03:10.460]  Ducky?\n",
      "[03:12.960 --> 03:13.920]  Ducky?\n",
      "[03:16.460 --> 03:17.420]  Ducky?\n",
      "[03:17.420 --> 03:17.520]  Ducky?\n",
      "[03:20.420 --> 03:20.660]  Yeah?\n",
      "[03:28.540 --> 03:29.940]  Yeah?\n",
      "[03:34.420 --> 03:36.480]  That one has kitties on it.\n",
      "[03:37.420 --> 03:37.740]  Kitties.\n",
      "[03:39.420 --> 03:40.300]  Kitties.\n",
      "[03:57.260 --> 03:57.700]  Yeah?\n",
      "[03:57.700 --> 03:59.360]  You're trying to stick your finger in the ball?\n",
      "[03:59.560 --> 04:00.100]  Look, look, look.\n",
      "[04:00.800 --> 04:01.940]  Let's count these, okay?\n",
      "[04:02.380 --> 04:04.020]  Oh, it fell apart.\n",
      "[04:06.800 --> 04:08.920]  There. Back together.\n",
      "[04:10.600 --> 04:11.100]  Let's see.\n",
      "[04:11.100 --> 04:17.060]  One, two, three, four, five, six, seven.\n",
      "[04:23.840 --> 04:26.360]  Good! It makes noise, look.\n",
      "[04:30.100 --> 04:31.900]  Good! Shake it!\n",
      "[04:35.940 --> 04:36.880]  Shake it!\n",
      "[04:38.220 --> 04:40.080]  Ooh, what should I have for you?\n",
      "[04:42.000 --> 04:43.780]  Good, you have your toys.\n",
      "[04:45.480 --> 04:46.000]  You trade?\n",
      "[04:46.440 --> 04:46.860]  Yeah.\n",
      "[04:53.200 --> 04:54.940]  Yeah, you're having fun.\n",
      "[04:56.460 --> 04:57.400]  Good, good.\n",
      "[04:57.400 --> 05:00.160]  He's staying pretty well in the bath either, he likes it?\n",
      "[05:00.600 --> 05:00.940]  Yeah.\n",
      "[05:01.000 --> 05:01.300]  Okay.\n",
      "[05:01.300 --> 05:01.960]  Hi, can you say?\n",
      "[05:05.440 --> 05:07.260]  SpongeBob, you found SpongeBob!\n",
      "[05:07.920 --> 05:08.440]  Look.\n",
      "[05:09.260 --> 05:10.720]  Look at the similarities.\n",
      "[05:11.100 --> 05:12.300]  Look, he looks like you.\n",
      "[05:13.580 --> 05:14.100]  SpongeBob.\n",
      "[05:14.600 --> 05:15.120]  Duggy.\n",
      "[05:17.740 --> 05:20.340]  You're so cute, you look just like SpongeBob.\n",
      "[05:21.200 --> 05:21.800]  Yes, you do.\n",
      "[05:22.600 --> 05:22.860]  Look.\n",
      "[05:23.920 --> 05:24.880]  It can be a ball.\n",
      "[05:25.880 --> 05:27.000]  Or a hat.\n",
      "[05:27.000 --> 05:27.060]  Or a hat.\n",
      "[05:30.160 --> 05:30.800]  Here.\n",
      "[05:31.980 --> 05:32.620]  Look, Lister.\n",
      "[05:34.360 --> 05:35.000]  Look.\n",
      "[05:36.080 --> 05:36.660]  Duggy, look.\n",
      "[05:38.780 --> 05:39.420]  Look.\n",
      "[05:42.180 --> 05:43.380]  You see it in there?\n",
      "[05:45.000 --> 05:45.580]  Can you see that?\n",
      "[05:47.920 --> 05:49.220]  Can you see that?\n",
      "[06:01.880 --> 06:02.640]  Look.\n",
      "[06:03.100 --> 06:03.200]  Look.\n",
      "[06:04.540 --> 06:06.460]  I'm making a tower.\n",
      "[06:07.400 --> 06:08.820]  You want to help me?\n",
      "[06:08.940 --> 06:09.100]  Look.\n",
      "[06:15.220 --> 06:16.740]  Look at this.\n",
      "[06:18.500 --> 06:19.940]  I want to go to the tower.\n",
      "[06:21.640 --> 06:22.260]  Hey!\n",
      "[06:22.260 --> 06:22.660]  Hey!\n",
      "[06:25.100 --> 06:26.240]  You go.\n",
      "[06:30.840 --> 06:32.280]  Okay, what does that do?\n",
      "[06:32.360 --> 06:32.660]  Orange.\n",
      "[06:33.740 --> 06:34.820]  Peach, I guess.\n",
      "[06:38.260 --> 06:38.340]  Peach.\n",
      "[06:39.020 --> 06:39.940]  Yellow.\n",
      "[06:41.080 --> 06:43.020]  Yellow, green.\n",
      "[06:44.160 --> 06:44.500]  Green.\n",
      "[06:44.500 --> 06:44.620]  Green.\n",
      "[06:50.120 --> 06:51.600]  These are stuck together.\n",
      "[06:54.060 --> 06:54.900]  Ta-da!\n",
      "[06:56.740 --> 06:59.000]  It's a beautiful rainbow tower.\n",
      "[07:00.400 --> 07:01.460]  I'm just going to eat the ducky.\n",
      "[07:01.800 --> 07:02.020]  Look.\n",
      "[07:03.500 --> 07:04.360]  Don't eat me.\n",
      "[07:07.260 --> 07:08.840]  Don't eat me, I'm a rubber ducky.\n",
      "[07:10.340 --> 07:10.600]  No.\n",
      "[07:14.500 --> 07:15.520]  Let's not eat him.\n",
      "[07:20.680 --> 07:21.920]  It's stuck in there.\n",
      "[07:22.020 --> 07:22.880]  Shake it.\n",
      "[07:25.060 --> 07:25.920]  Come on, eat it.\n",
      "[07:26.880 --> 07:27.940]  Good one!\n",
      "[07:30.500 --> 07:30.640]  Good.\n",
      "[07:34.500 --> 07:34.780]  It's open.\n",
      "[07:38.500 --> 07:39.200]  You're going to die.\n",
      "[07:42.260 --> 07:43.600]  Ducky!\n",
      "[07:44.640 --> 07:45.960]  There's the ducky.\n",
      "[07:47.900 --> 07:50.040]  Hey, say ducky.\n",
      "[07:51.880 --> 07:52.660]  Say ducky.\n",
      "[07:53.760 --> 07:54.860]  Ducky.\n",
      "[07:57.140 --> 07:58.620]  Good one!\n",
      "[07:58.820 --> 08:00.220]  You talk so much.\n",
      "[08:14.280 --> 08:14.920]  Ducky.\n",
      "[08:14.920 --> 08:15.500]  You look like a duck.\n",
      "[08:17.020 --> 08:17.860]  Eat it.\n",
      "[08:18.180 --> 08:18.340]  Ducky.\n",
      "[08:21.080 --> 08:21.660]  Good one.\n",
      "[08:42.100 --> 08:43.060]  Good one.\n",
      "[08:46.320 --> 08:48.200]  Pick up the red one, ducky.\n",
      "[08:50.060 --> 08:51.080]  Where's the red one?\n",
      "[08:51.220 --> 08:51.740]  Can you get it?\n",
      "[08:52.960 --> 08:54.300]  Can you get the red one?\n",
      "[08:56.380 --> 08:57.500]  Pick it up.\n",
      "[08:58.120 --> 08:58.820]  Let's see ducky.\n",
      "[09:00.540 --> 09:01.560]  Get the red one.\n",
      "[09:04.880 --> 09:06.080]  You're trying to find it.\n",
      "[09:06.080 --> 09:06.980]  Okay, here he is.\n",
      "[09:08.060 --> 09:09.620]  You really love that ducky.\n",
      "[09:13.520 --> 09:15.180]  Here he is!\n",
      "[09:16.080 --> 09:17.320]  Ducky with peek-a-boo.\n",
      "[09:19.580 --> 09:20.480]  He's here.\n",
      "[09:22.560 --> 09:24.080]  I'm going to make him stuck inside.\n",
      "[09:28.300 --> 09:29.780]  He just likes the ducky.\n",
      "[09:31.340 --> 09:33.480]  I told him to say ducky\n",
      "[09:33.480 --> 09:35.280]  and he did it perfectly for the camera.\n",
      "[09:35.860 --> 09:36.960]  You did it!\n",
      "[09:37.380 --> 09:37.900]  Ducky.\n",
      "[09:40.020 --> 09:40.980]  That's fun.\n",
      "[09:42.740 --> 09:43.620]  These are cute.\n",
      "[09:43.760 --> 09:44.260]  I like them.\n",
      "[09:47.420 --> 09:49.200]  What were you wanting ducky to?\n",
      "[09:50.840 --> 09:53.520]  If you want the back, let me know\n",
      "[09:53.520 --> 09:54.280]  and I can bring it back.\n",
      "[09:55.740 --> 09:56.780]  Oh, look.\n",
      "[09:57.240 --> 09:58.560]  It's a pirate.\n",
      "[09:59.040 --> 10:00.480]  It goes in his ship.\n",
      "[10:00.480 --> 10:01.120]  Oh, look.\n",
      "[10:01.200 --> 10:02.220]  There's a parrot.\n",
      "[10:12.260 --> 10:13.700]  That's a parrot.\n",
      "[10:15.620 --> 10:16.380]  Parrot.\n",
      "[10:18.480 --> 10:19.340]  That's a pirate.\n",
      "[10:19.880 --> 10:21.060]  It will start with P.\n",
      "[10:31.140 --> 10:33.160]  You just want to eat everything?\n",
      "[10:33.920 --> 10:34.840]  It's a big piece.\n",
      "[10:35.300 --> 10:36.380]  Hi ducky.\n",
      "[10:36.620 --> 10:38.840]  I'm a turtle and I swim through the water.\n",
      "[10:40.820 --> 10:42.780]  He's the same color as your shirt.\n",
      "[10:45.700 --> 10:46.480]  You're a turtle.\n",
      "[10:50.700 --> 10:52.640]  Do you like the parrot?\n",
      "[10:54.700 --> 10:56.180]  Do you like the parrot?\n",
      "[10:57.400 --> 10:58.800]  Do you like the parrot?\n",
      "[11:00.480 --> 11:02.020]  Do you like the parrot?\n",
      "[11:06.440 --> 11:07.060]  Pirate.\n",
      "[11:14.300 --> 11:15.640]  Let's put him in the boat.\n",
      "[11:20.960 --> 11:21.940]  Another pirate.\n",
      "[11:22.160 --> 11:22.540]  Where did he go?\n",
      "[11:31.060 --> 11:32.500]  There he is.\n",
      "[11:32.680 --> 11:33.360]  Look.\n",
      "[11:33.440 --> 11:34.640]  He's a cute pirate.\n",
      "[11:36.060 --> 11:37.180]  He's very cute.\n",
      "[11:53.320 --> 11:55.520]  He's a turtle.\n",
      "[11:56.980 --> 11:57.980]  He's a turtle.\n",
      "[11:57.980 --> 11:58.120]  Hi, turtle.\n",
      "[11:58.840 --> 11:59.800]  Hi, ducky.\n",
      "[12:01.700 --> 12:03.260]  Hi, Mr. Turtle.\n",
      "[12:05.980 --> 12:06.420]  Say turtle.\n",
      "[12:07.620 --> 12:08.580]  Turtle.\n",
      "[12:11.980 --> 12:13.320]  That's so good, ducky.\n",
      "[12:14.320 --> 12:15.360]  Say parrot.\n",
      "[12:18.680 --> 12:19.380]  Parrot.\n",
      "[12:22.200 --> 12:23.200]  Say parrot.\n",
      "[12:25.800 --> 12:26.800]  Say birdie.\n",
      "[12:26.800 --> 12:26.800]  Say birdie.\n",
      "[12:36.720 --> 12:37.760]  You get it.\n",
      "[12:48.200 --> 12:49.540]  Do you have an interest in the boat?\n",
      "[12:54.240 --> 12:56.820]  You're so cute.\n",
      "[12:59.640 --> 13:01.780]  Hi, Mr. Ducky.\n",
      "[13:03.200 --> 13:03.760]  Give me a kiss.\n",
      "[13:04.860 --> 13:07.080]  I love you.\n",
      "[13:17.160 --> 13:19.280]  You're a turtle.\n",
      "[13:23.200 --> 13:24.360]  Have fun.\n",
      "\n",
      "Transcription Statistics:\n",
      "Total segments: 208\n",
      "\n",
      "First few transcriptions:\n",
      "  start_time end_time                                     text\n",
      "0    0:00:00  0:00:02                  It's just Dougie, so...\n",
      "1    0:00:02  0:00:02                          Come here, you.\n",
      "2    0:00:03  0:00:04  ...if you don't mind watching him play.\n",
      "3    0:00:06  0:00:08                   Oh no, look what I do!\n",
      "4    0:00:13  0:00:15               The camera is really fast.\n"
     ]
    }
   ],
   "source": [
    "##whisper transcription without speaker recognition\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"Transcribe audio using Whisper\"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    model = whisper.load_model(\"large-v2\", device=\"cpu\")\n",
    "    \n",
    "    # Keep the parent-infant context in the prompt but make it more transcription-focused\n",
    "    initial_prompt = \"\"\"This is a parent-infant interaction recording. \"\"\"\n",
    "#    Please transcribe all speech, including infant vocalizations, parent speech, and any notable sounds.\n",
    "#    Include both verbal and non-verbal vocalizations.\"\"\"\n",
    "    \n",
    "    print(\"Transcribing...\")\n",
    "    result = model.transcribe(\n",
    "        audio_path,\n",
    "        word_timestamps=True,\n",
    "        verbose=True,\n",
    "        initial_prompt=initial_prompt,\n",
    "        language=\"en\",\n",
    "        task=\"transcribe\"\n",
    "    )\n",
    "    \n",
    "    # Process segments\n",
    "    segments = []\n",
    "    \n",
    "    # First pass: Get clean transcriptions with timing\n",
    "    for segment in result[\"segments\"]:\n",
    "        text = segment[\"text\"].strip()\n",
    "        start = segment[\"start\"]\n",
    "        end = segment[\"end\"]\n",
    "        duration = end - start\n",
    "        \n",
    "        # Skip empty segments\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        segment_info = {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'start_time': str(timedelta(seconds=int(start))),\n",
    "            'end_time': str(timedelta(seconds=int(end))),\n",
    "            'text': text,\n",
    "            'duration': duration\n",
    "        }\n",
    "        \n",
    "        segments.append(segment_info)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(segments)\n",
    "    \n",
    "    return df, result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df, result = transcribe_audio(\n",
    "            \"/Users/yueyan/Documents/project/wearable/media/12mon/raw/IW_038_12_TL.wav\"\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        df.to_csv(\"/Users/yueyan/Documents/project/wearable/transcription/whisper/12mon/IW_038_12_whisper_wopre_wos_111924.csv\", index=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nTranscription Statistics:\")\n",
    "        print(f\"Total segments: {len(df)}\")\n",
    "        \n",
    "        print(\"\\nFirst few transcriptions:\")\n",
    "        print(df[['start_time', 'end_time', 'text']].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c86f1685-fc99-455a-88ed-239537ee6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "###load datasets\n",
    "whisper_df = pd.read_csv(\"/Users/yueyan/Documents/project/wearable/transcription/whisper/12mon/IW_038_12_whisper_wopre_wos_111924.csv\")\n",
    "human_df = pd.read_csv(\"/Users/yueyan/Documents/project/wearable/transcription/human/12mon/IW_038_12_human_transcription.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be6b912-4d90-4314-b6c1-cfe298211fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing segments: 180it [00:00, 3118.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison Summary:\n",
      "Total segments analyzed: 121\n",
      "Average accuracy: 74.2%\n",
      "Average similarity score: 0.806\n",
      "High quality matches (>80%): 72\n",
      "Low quality matches (<60%): 39\n",
      "\n",
      "Sample comparison results:\n",
      "  Time Range  Human Transcription  \\\n",
      "0  37.2-38.4          oh squeaky.   \n",
      "1  46.5-47.9    oh did it tickle?   \n",
      "2  46.5-47.9    oh did it tickle?   \n",
      "3  48.4-49.5  did it tickle here.   \n",
      "4  52.1-52.3             come on.   \n",
      "\n",
      "                          Whisper Transcription  Accuracy (%)  \n",
      "0                                  Oh, squeaky!         100.0  \n",
      "1                            Oh, did it tickle?         100.0  \n",
      "2  Did it tickle? Here, got it? Oh, you got it!         100.0  \n",
      "3  Did it tickle? Here, got it? Oh, you got it!         100.0  \n",
      "4                                      Come on.         100.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "class EnhancedTranscriptionComparator:\n",
    "    def __init__(self, whisper_df: pd.DataFrame, human_df: pd.DataFrame):\n",
    "        \"\"\"Initialize with enhanced text cleaning and comparison capabilities\"\"\"\n",
    "        self.whisper_data = whisper_df.copy()\n",
    "        self.human_data = human_df.copy()\n",
    "        \n",
    "        # Pre-clean the data\n",
    "        self.human_data = self.human_data[self.human_data['text'].notna()]\n",
    "        if 'end ' in self.human_data.columns:\n",
    "            self.human_data = self.human_data.rename(columns={'end ': 'end'})\n",
    "        \n",
    "        # Convert times to numeric\n",
    "        self.human_data['start'] = pd.to_numeric(self.human_data['start'])\n",
    "        self.human_data['end'] = pd.to_numeric(self.human_data['end'])\n",
    "        \n",
    "        # Clean text and store originals\n",
    "        self.whisper_data['clean_text'] = self.whisper_data['text'].apply(self.clean_text)\n",
    "        self.human_data['clean_text'] = self.human_data['text'].apply(self.clean_text)\n",
    "        self.whisper_data['original_text'] = self.whisper_data['text']\n",
    "        self.human_data['original_text'] = self.human_data['text']\n",
    "        \n",
    "        # Find overlap range\n",
    "        self.overlap_start = max(self.whisper_data['start'].min(), self.human_data['start'].min())\n",
    "        self.overlap_end = min(self.whisper_data['end'].max(), self.human_data['end'].max())\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"Enhanced text cleaning function\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "            \n",
    "        text = str(text).lower()\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        replacements = {\n",
    "            'uhm': 'um', 'uhh': 'uh', 'hmm': 'hm',\n",
    "            'mhm': 'mm', 'yeah': 'yes', 'yah': 'yes',\n",
    "            'nah': 'no'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = re.sub(r'\\b' + old + r'\\b', new, text)\n",
    "            \n",
    "        return text\n",
    "\n",
    "    def get_word_metrics(self, human_text: str, whisper_text: str) -> Dict[str, int]:\n",
    "        \"\"\"Calculate detailed word-level metrics\"\"\"\n",
    "        human_words = set(self.clean_text(human_text).split())\n",
    "        whisper_words = set(self.clean_text(whisper_text).split())\n",
    "        \n",
    "        correct_words = len(human_words.intersection(whisper_words))\n",
    "        total_words_human = len(human_words)\n",
    "        total_words_whisper = len(whisper_words)\n",
    "        mismatches = max(total_words_human, total_words_whisper) - correct_words\n",
    "        \n",
    "        return {\n",
    "            'word_count_human': total_words_human,\n",
    "            'word_count_whisper': total_words_whisper,\n",
    "            'correct_words': correct_words,\n",
    "            'mismatches': mismatches\n",
    "        }\n",
    "\n",
    "    def generate_comparison_results(self, time_tolerance: float = 0.5, \n",
    "                                  similarity_threshold: float = 0.3) -> pd.DataFrame:\n",
    "        \"\"\"Generate comprehensive comparison results for spreadsheet\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        human_segments = self.human_data[\n",
    "            (self.human_data['start'] >= self.overlap_start) &\n",
    "            (self.human_data['end'] <= self.overlap_end)\n",
    "        ]\n",
    "        \n",
    "        whisper_segments = self.whisper_data[\n",
    "            (self.whisper_data['start'] >= self.overlap_start) &\n",
    "            (self.whisper_data['end'] <= self.overlap_end)\n",
    "        ]\n",
    "        \n",
    "        for _, human_seg in tqdm(human_segments.iterrows(), desc=\"Analyzing segments\"):\n",
    "            potential_matches = whisper_segments[\n",
    "                (whisper_segments['start'] >= human_seg['start'] - time_tolerance) &\n",
    "                (whisper_segments['start'] <= human_seg['end'] + time_tolerance)\n",
    "            ]\n",
    "            \n",
    "            for _, whisper_seg in potential_matches.iterrows():\n",
    "                similarity = SequenceMatcher(\n",
    "                    None,\n",
    "                    human_seg['clean_text'],\n",
    "                    whisper_seg['clean_text']\n",
    "                ).ratio()\n",
    "                \n",
    "                if similarity > similarity_threshold:\n",
    "                    # Get word-level metrics\n",
    "                    word_metrics = self.get_word_metrics(\n",
    "                        human_seg['original_text'],\n",
    "                        whisper_seg['original_text']\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    accuracy = (word_metrics['correct_words'] / word_metrics['word_count_human'] * 100) \\\n",
    "                        if word_metrics['word_count_human'] > 0 else 0\n",
    "                    \n",
    "                    # Generate comments\n",
    "                    comments = []\n",
    "                    if similarity < 0.5:\n",
    "                        comments.append(\"Low similarity score\")\n",
    "                    if abs(word_metrics['word_count_human'] - word_metrics['word_count_whisper']) > 3:\n",
    "                        comments.append(\"Significant word count difference\")\n",
    "                    if accuracy < 60:\n",
    "                        comments.append(\"Low accuracy\")\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Time Range': f\"{human_seg['start']:.1f}-{human_seg['end']:.1f}\",\n",
    "                        'Whisper Transcription': whisper_seg['original_text'],\n",
    "                        'Human Transcription': human_seg['original_text'],\n",
    "                        'SequenceMatcher Score': round(similarity, 3),\n",
    "                        'Word Count (Human)': word_metrics['word_count_human'],\n",
    "                        'Word Count (Whisper)': word_metrics['word_count_whisper'],\n",
    "                        'Correct Words': word_metrics['correct_words'],\n",
    "                        'Mismatches': word_metrics['mismatches'],\n",
    "                        'Accuracy (%)': round(accuracy, 1),\n",
    "                        'Comments/Notes': \"; \".join(comments) if comments else \"OK\"\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "def compare_and_export(whisper_file: pd.DataFrame, human_file: pd.DataFrame, \n",
    "                      output_path: str = \"transcription_comparison.csv\") -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"Compare transcriptions and export results\"\"\"\n",
    "    # Initialize comparator\n",
    "    comparator = EnhancedTranscriptionComparator(whisper_file, human_file)\n",
    "    \n",
    "    # Generate comparison results\n",
    "    results_df = comparator.generate_comparison_results()\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    stats = {\n",
    "        'Total Segments': len(results_df),\n",
    "        'Average Accuracy': results_df['Accuracy (%)'].mean(),\n",
    "        'Average Similarity': results_df['SequenceMatcher Score'].mean(),\n",
    "        'High Quality Matches': len(results_df[results_df['Accuracy (%)'] > 80]),\n",
    "        'Low Quality Matches': len(results_df[results_df['Accuracy (%)'] < 60])\n",
    "    }\n",
    "    \n",
    "    # Export to CSV\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nComparison Summary:\")\n",
    "    print(f\"Total segments analyzed: {stats['Total Segments']}\")\n",
    "    print(f\"Average accuracy: {stats['Average Accuracy']:.1f}%\")\n",
    "    print(f\"Average similarity score: {stats['Average Similarity']:.3f}\")\n",
    "    print(f\"High quality matches (>80%): {stats['High Quality Matches']}\")\n",
    "    print(f\"Low quality matches (<60%): {stats['Low Quality Matches']}\")\n",
    "    \n",
    "    return results_df, stats\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume whisper_df and human_df are your input DataFrames; replace with your desired path for the comparison results to be saved\n",
    "    results_df, stats = compare_and_export(whisper_df, human_df, \"/Users/yueyan/Documents/project/wearable/transcription/comparison/12mon/IW_038_12_transcription_comparison.csv\")\n",
    "    \n",
    "    # Display first few results\n",
    "    print(\"\\nSample comparison results:\")\n",
    "    print(results_df[['Time Range', 'Human Transcription', \n",
    "                     'Whisper Transcription', 'Accuracy (%)']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109b288-71ba-4f5e-b7e0-a2df248ba215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
